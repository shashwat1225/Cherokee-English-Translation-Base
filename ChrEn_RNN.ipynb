{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.utils\n",
        "import torch.nn.functional as func\n",
        "\n",
        "from typing import List, Tuple, Optional\n",
        "from collections import namedtuple\n",
        "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence"
      ],
      "metadata": {
        "id": "t1qGu6xmpmxx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rXVrtnbPo1s2"
      },
      "outputs": [],
      "source": [
        "class ModelEmbeddings(nn.Module):\n",
        "    \"\"\"\n",
        "    Class that converts input words to their embeddings.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, embed_size, vocab):\n",
        "        \"\"\"\n",
        "        Init the Embedding layers.\n",
        "\n",
        "        @param embed_size (int): Embedding size (dimensionality)\n",
        "        @param vocab (Vocab): Vocabulary object containing src and tgt languages\n",
        "                              See vocab.py for documentation.\n",
        "        \"\"\"\n",
        "        super(ModelEmbeddings, self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "\n",
        "        # default values\n",
        "        self.source = None\n",
        "        self.target = None\n",
        "\n",
        "        src_pad_token_idx = vocab.src[\"<pad>\"]\n",
        "        tgt_pad_token_idx = vocab.tgt[\"<pad>\"]\n",
        "\n",
        "        self.source = nn.Embedding(\n",
        "            len(vocab.src),\n",
        "            self.embed_size,\n",
        "            padding_idx=src_pad_token_idx,\n",
        "        )\n",
        "        self.target = nn.Embedding(\n",
        "            len(vocab.tgt),\n",
        "            self.embed_size,\n",
        "            padding_idx=tgt_pad_token_idx,\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "Hypothesis = namedtuple(\"Hypothesis\", [\"value\", \"score\"])\n",
        "\n",
        "\n",
        "class NMT(nn.Module):\n",
        "    \"\"\"Simple Neural Machine Translation Model:\n",
        "    - Bidirectional LSTM Encoder\n",
        "    - Unidirectional LSTM Decoder\n",
        "    - Global Attention Model (Luong, et al. 2015)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, embed_size, hidden_size, vocab, dropout_rate=0.2):\n",
        "        \"\"\"Init NMT Model.\n",
        "\n",
        "        @param embed_size (int): Embedding size (dimensionality)\n",
        "        @param hidden_size (int): Hidden Size, the size of hidden states (dimensionality)\n",
        "        @param vocab (Vocab): Vocabulary object containing src and tgt languages\n",
        "                              See vocab.py for documentation.\n",
        "        @param dropout_rate (float): Dropout probability, for attention\n",
        "        \"\"\"\n",
        "        super(NMT, self).__init__()\n",
        "        self.model_embeddings = ModelEmbeddings(embed_size, vocab)\n",
        "        self.hidden_size = hidden_size\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.vocab = vocab\n",
        "\n",
        "        # For sanity check only, not relevant to implementation\n",
        "        self.gen_sanity_check = False\n",
        "        self.counter = 0\n",
        "\n",
        "        self.encoder = torch.nn.LSTM(\n",
        "            input_size=embed_size,\n",
        "            hidden_size=self.hidden_size,\n",
        "            bias=True,\n",
        "            bidirectional=True,\n",
        "        )\n",
        "        self.decoder = torch.nn.LSTMCell(\n",
        "            input_size=embed_size + hidden_size,\n",
        "            hidden_size=self.hidden_size,\n",
        "            bias=True,\n",
        "        )\n",
        "        self.h_projection = torch.nn.Linear(\n",
        "            in_features=2 * self.hidden_size, out_features=self.hidden_size, bias=False\n",
        "        )\n",
        "        self.c_projection = torch.nn.Linear(\n",
        "            in_features=2 * self.hidden_size, out_features=self.hidden_size, bias=False\n",
        "        )\n",
        "        self.att_projection = torch.nn.Linear(\n",
        "            in_features=2 * self.hidden_size, out_features=self.hidden_size, bias=False\n",
        "        )\n",
        "        self.combined_output_projection = torch.nn.Linear(\n",
        "            in_features=3 * self.hidden_size, out_features=self.hidden_size, bias=False\n",
        "        )\n",
        "        self.target_vocab_projection = torch.nn.Linear(\n",
        "            in_features=self.hidden_size, out_features=len(self.vocab.tgt), bias=False\n",
        "        )\n",
        "        self.dropout = torch.nn.Dropout(p=self.dropout_rate)\n",
        "\n",
        "    def forward(self, source: List[List[str]], target: List[List[str]]) -> torch.Tensor:\n",
        "        \"\"\"Take a mini-batch of source and target sentences, compute the log-likelihood of\n",
        "        target sentences under the language models learned by the NMT system.\n",
        "\n",
        "        @param source: list of source sentence tokens\n",
        "        @param target: list of target sentence tokens, wrapped by `<s>` and `</s>`\n",
        "\n",
        "        @returns scores (Tensor): a variable/tensor of shape (b, ) representing the\n",
        "                                    log-likelihood of generating the gold-standard target sentence for\n",
        "                                    each example in the input batch. Here b = batch size.\n",
        "        \"\"\"\n",
        "        # Compute sentence lengths\n",
        "        source_lengths = [len(s) for s in source]\n",
        "\n",
        "        # Convert list of lists into tensors\n",
        "        source_padded = self.vocab.src.to_input_tensor(\n",
        "            source, device=self.device\n",
        "        )  # Tensor: (src_len, b)\n",
        "        target_padded = self.vocab.tgt.to_input_tensor(\n",
        "            target, device=self.device\n",
        "        )  # Tensor: (tgt_len, b)\n",
        "        enc_hiddens, dec_init_state = self.encode(source_padded, source_lengths)\n",
        "        enc_masks = self.generate_sent_masks(enc_hiddens, source_lengths)\n",
        "        combined_outputs = self.decode(\n",
        "            enc_hiddens, enc_masks, dec_init_state, target_padded\n",
        "        )\n",
        "        P = func.log_softmax(self.target_vocab_projection(combined_outputs), dim=-1)\n",
        "\n",
        "        # Zero out, probabilities for which we have nothing in the target text\n",
        "        target_masks = (target_padded != self.vocab.tgt[\"<pad>\"]).float()\n",
        "\n",
        "        # Compute log probability of generating true target words\n",
        "        target_gold_words_log_prob = (\n",
        "            torch.gather(P, index=target_padded[1:].unsqueeze(-1), dim=-1).squeeze(-1)\n",
        "            * target_masks[1:]\n",
        "        )\n",
        "        scores = target_gold_words_log_prob.sum(dim=0)\n",
        "        return scores\n",
        "\n",
        "    def encode(\n",
        "        self, source_padded: torch.Tensor, source_lengths: List[int]\n",
        "    ) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n",
        "        \"\"\"Apply the encoder to source sentences to obtain encoder hidden states.\n",
        "            Additionally, take the final states of the encoder and project them to obtain initial states for decoder.\n",
        "\n",
        "        @param source_padded: Tensor of padded source sentences with shape (src_len, b), where\n",
        "                              b = batch_size, src_len = maximum source sentence length. Note that\n",
        "                              these have already been sorted in order of longest to shortest sentence.\n",
        "        @param source_lengths: List of actual lengths for each of the source sentences in the batch\n",
        "        @returns enc_hiddens: Tensor of hidden units with shape (b, src_len, h*2), where\n",
        "                              b = batch size, src_len = maximum source sentence length, h = hidden size.\n",
        "        @returns dec_init_state (tuple(Tensor, Tensor)): Tuple of tensors representing the decoder's initial\n",
        "                                                         hidden state and cell.\n",
        "        \"\"\"\n",
        "        X = self.model_embeddings.source(source_padded)\n",
        "\n",
        "        X_packed = pack_padded_sequence(X, source_lengths)\n",
        "        enc_hiddens, (last_hidden, last_cell) = self.encoder(X_packed)\n",
        "        enc_hiddens, _ = pad_packed_sequence(enc_hiddens, batch_first=True)\n",
        "\n",
        "        concat_last_hidden = torch.cat((last_hidden[0], last_hidden[1]), 1)\n",
        "        init_decoder_hidden = self.h_projection(concat_last_hidden)\n",
        "\n",
        "        concat_last_cell = torch.cat((last_cell[0], last_cell[1]), 1)\n",
        "        init_decoder_cell = self.c_projection(concat_last_cell)\n",
        "\n",
        "        dec_init_state = (init_decoder_hidden, init_decoder_cell)\n",
        "\n",
        "        return enc_hiddens, dec_init_state\n",
        "\n",
        "    def decode(\n",
        "        self,\n",
        "        enc_hiddens: torch.Tensor,\n",
        "        enc_masks: torch.Tensor,\n",
        "        dec_init_state: Tuple[torch.Tensor, torch.Tensor],\n",
        "        target_padded: torch.Tensor,\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"Compute combined output vectors for a batch.\n",
        "\n",
        "        @param enc_hiddens: Hidden states (b, src_len, h*2), where\n",
        "                                     b = batch size, src_len = maximum source sentence length, h = hidden size.\n",
        "        @param enc_masks: Tensor of sentence masks (b, src_len), where\n",
        "                          b = batch size, src_len = maximum source sentence length.\n",
        "        @param dec_init_state: Initial state and cell for decoder\n",
        "        @param target_padded: Gold-standard padded target sentences (tgt_len, b), where\n",
        "                              tgt_len = maximum target sentence length, b = batch size.\n",
        "\n",
        "        @returns combined_outputs: combined output tensor  (tgt_len, b,  h), where\n",
        "                                   tgt_len = maximum target sentence length, b = batch_size,  h = hidden size\n",
        "        \"\"\"\n",
        "        # Chop of the <END> token for max length sentences.\n",
        "        target_padded = target_padded[:-1]\n",
        "\n",
        "        # Initialize the decoder state (hidden and cell)\n",
        "        dec_state = dec_init_state\n",
        "\n",
        "        # Initialize previous combined output vector o_{t-1} as zero\n",
        "        batch_size = enc_hiddens.size(0)\n",
        "        o_prev = torch.zeros(batch_size, self.hidden_size, device=self.device)\n",
        "\n",
        "        # Initialize a list we will use to collect the combined output o_t on each step\n",
        "        combined_outputs = []\n",
        "\n",
        "        enc_hiddens_proj = self.att_projection(enc_hiddens)\n",
        "        Y = self.model_embeddings.target(target_padded)\n",
        "        for Y_t in torch.split(Y, 1):\n",
        "            Y_t_squeezed = torch.squeeze(Y_t)\n",
        "            Y_bar_t = torch.cat([Y_t_squeezed, o_prev], 1)\n",
        "            dec_state, o_t, e_t = self.step(\n",
        "                Ybar_t=Y_bar_t,\n",
        "                dec_state=dec_state,\n",
        "                enc_hiddens=enc_hiddens,\n",
        "                enc_hiddens_proj=enc_hiddens_proj,\n",
        "                enc_masks=enc_masks,\n",
        "            )\n",
        "            combined_outputs.append(o_t)\n",
        "            o_prev = o_t\n",
        "\n",
        "        combined_outputs = torch.stack(combined_outputs, dim=0)\n",
        "\n",
        "        return combined_outputs\n",
        "\n",
        "    def step(\n",
        "        self,\n",
        "        Ybar_t: torch.Tensor,\n",
        "        dec_state: Tuple[torch.Tensor, torch.Tensor],\n",
        "        enc_hiddens: torch.Tensor,\n",
        "        enc_hiddens_proj: torch.Tensor,\n",
        "        enc_masks: Optional[torch.Tensor],\n",
        "    ) -> Tuple[Tuple, torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"Compute one forward step of the LSTM decoder, including the attention computation.\n",
        "\n",
        "        @param Ybar_t: Concatenated Tensor of [Y_t o_prev], with shape (b, e + h). The input for the decoder,\n",
        "                       where b = batch size, e = embedding size, h = hidden size.\n",
        "        @param dec_state: Tuple of tensors both with shape (b, h), where b = batch size, h = hidden size.\n",
        "                          First tensor is decoder's prev hidden state, second tensor is decoder's prev cell.\n",
        "        @param enc_hiddens: Encoder hidden states Tensor, with shape (b, src_len, h * 2), where b = batch size,\n",
        "                            src_len = maximum source length, h = hidden size.\n",
        "        @param enc_hiddens_proj: Encoder hidden states Tensor, projected from (h * 2) to h. Tensor is with shape (b, src_len, h),\n",
        "                                 where b = batch size, src_len = maximum source length, h = hidden size.\n",
        "        @param enc_masks: Tensor of sentence masks shape (b, src_len),\n",
        "                          where b = batch size, src_len is maximum source length.\n",
        "\n",
        "        @returns dec_state (tuple (Tensor, Tensor)): Tuple of tensors both shape (b, h), where b = batch size, h = hidden size.\n",
        "                First tensor is decoder's new hidden state, second tensor is decoder's new cell.\n",
        "        @returns combined_output (Tensor): Combined output Tensor at time step t, shape (b, h), where b = batch size, h = hidden size.\n",
        "        @returns e_t (Tensor): Tensor of shape (b, src_len). It is attention scores distribution.\n",
        "                                Note: You will not use this outside of this function.\n",
        "                                      We are simply returning this value so that we can sanity check\n",
        "                                      your implementation.\n",
        "        \"\"\"\n",
        "\n",
        "        dec_state = self.decoder(Ybar_t, dec_state)\n",
        "        dec_hidden, dec_cell = dec_state\n",
        "\n",
        "        e_t = torch.squeeze(\n",
        "            torch.bmm(enc_hiddens_proj, torch.unsqueeze(dec_hidden, dim=2)), dim=2\n",
        "        )\n",
        "\n",
        "        # Set e_t to -inf where enc_masks has 1\n",
        "        if enc_masks is not None:\n",
        "            e_t.data.masked_fill_(enc_masks.bool(), -float(\"inf\"))\n",
        "\n",
        "        alpha_t = func.softmax(e_t, dim=-1)\n",
        "\n",
        "        a_t = torch.squeeze(\n",
        "            torch.bmm(torch.unsqueeze(alpha_t, dim=1), enc_hiddens), dim=1\n",
        "        )\n",
        "\n",
        "        U_t = torch.cat([dec_hidden, a_t], dim=1)\n",
        "        V_t = self.combined_output_projection(U_t)\n",
        "        O_t = self.dropout(torch.tanh(V_t))\n",
        "\n",
        "        combined_output = O_t\n",
        "        return dec_state, combined_output, e_t\n",
        "\n",
        "    def generate_sent_masks(\n",
        "        self, enc_hiddens: torch.Tensor, source_lengths: List[int]\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"Generate sentence masks for encoder hidden states.\n",
        "\n",
        "        @param enc_hiddens: encodings of shape (b, src_len, 2*h), where b = batch size,\n",
        "                            src_len = max source length, h = hidden size.\n",
        "        @param source_lengths: List of actual lengths for each of the sentences in the batch.\n",
        "\n",
        "        @returns enc_masks: Tensor of sentence masks of shape (b, src_len),\n",
        "                            where src_len = max source length, h = hidden size.\n",
        "        \"\"\"\n",
        "        enc_masks = torch.zeros(\n",
        "            enc_hiddens.size(0), enc_hiddens.size(1), dtype=torch.float\n",
        "        )\n",
        "        for e_id, src_len in enumerate(source_lengths):\n",
        "            enc_masks[e_id, src_len:] = 1\n",
        "        return enc_masks.to(self.device)\n",
        "\n",
        "    def beam_search(\n",
        "        self, src_sent: List[str], beam_size: int = 5, max_decoding_time_step: int = 70\n",
        "    ) -> List[Hypothesis]:\n",
        "        \"\"\"Given a single source sentence, perform beam search, yielding translations in the target language.\n",
        "        @param src_sent: a single source sentence (words)\n",
        "        @param beam_size: beam size\n",
        "        @param max_decoding_time_step: maximum number of time steps to unroll the decoding RNN\n",
        "        @returns hypotheses: a list of hypothesis, each hypothesis has two fields:\n",
        "                value: List[str]: the decoded target sentence, represented as a list of words\n",
        "                score: float: the log-likelihood of the target sentence\n",
        "        \"\"\"\n",
        "        src_sents_var = self.vocab.src.to_input_tensor([src_sent], self.device)\n",
        "\n",
        "        src_encodings, dec_init_vec = self.encode(src_sents_var, [len(src_sent)])\n",
        "        src_encodings_att_linear = self.att_projection(src_encodings)\n",
        "\n",
        "        h_tm1 = dec_init_vec\n",
        "        att_tm1 = torch.zeros(1, self.hidden_size, device=self.device)\n",
        "\n",
        "        eos_id = self.vocab.tgt[\"</s>\"]\n",
        "\n",
        "        hypotheses = [[\"<s>\"]]\n",
        "        hyp_scores = torch.zeros(len(hypotheses), dtype=torch.float, device=self.device)\n",
        "        completed_hypotheses = []\n",
        "\n",
        "        t = 0\n",
        "        while len(completed_hypotheses) < beam_size and t < max_decoding_time_step:\n",
        "            t += 1\n",
        "            hyp_num = len(hypotheses)\n",
        "\n",
        "            exp_src_encodings = src_encodings.expand(\n",
        "                hyp_num, src_encodings.size(1), src_encodings.size(2)\n",
        "            )\n",
        "\n",
        "            exp_src_encodings_att_linear = src_encodings_att_linear.expand(\n",
        "                hyp_num,\n",
        "                src_encodings_att_linear.size(1),\n",
        "                src_encodings_att_linear.size(2),\n",
        "            )\n",
        "\n",
        "            y_tm1 = torch.tensor(\n",
        "                [self.vocab.tgt[hyp[-1]] for hyp in hypotheses],\n",
        "                dtype=torch.long,\n",
        "                device=self.device,\n",
        "            )\n",
        "            y_t_embed = self.model_embeddings.target(y_tm1)\n",
        "\n",
        "            x = torch.cat([y_t_embed, att_tm1], dim=-1)\n",
        "\n",
        "            (h_t, cell_t), att_t, _ = self.step(\n",
        "                x,\n",
        "                h_tm1,\n",
        "                exp_src_encodings,\n",
        "                exp_src_encodings_att_linear,\n",
        "                enc_masks=None,\n",
        "            )\n",
        "\n",
        "            # log probabilities over target words\n",
        "            log_p_t = func.log_softmax(self.target_vocab_projection(att_t), dim=-1)\n",
        "\n",
        "            live_hyp_num = beam_size - len(completed_hypotheses)\n",
        "            continuing_hyp_scores = (\n",
        "                hyp_scores.unsqueeze(1).expand_as(log_p_t) + log_p_t\n",
        "            ).view(-1)\n",
        "            top_cand_hyp_scores, top_cand_hyp_pos = torch.topk(\n",
        "                continuing_hyp_scores, k=live_hyp_num\n",
        "            )\n",
        "\n",
        "            prev_hyp_ids = top_cand_hyp_pos // len(self.vocab.tgt)\n",
        "            hyp_word_ids = top_cand_hyp_pos % len(self.vocab.tgt)\n",
        "\n",
        "            new_hypotheses = []\n",
        "            live_hyp_ids = []\n",
        "            new_hyp_scores = []\n",
        "\n",
        "            for prev_hyp_id, hyp_word_id, cand_new_hyp_score in zip(\n",
        "                prev_hyp_ids, hyp_word_ids, top_cand_hyp_scores\n",
        "            ):\n",
        "                prev_hyp_id = prev_hyp_id.item()\n",
        "                hyp_word_id = hyp_word_id.item()\n",
        "                cand_new_hyp_score = cand_new_hyp_score.item()\n",
        "\n",
        "                hyp_word = self.vocab.tgt.id2word[hyp_word_id]\n",
        "                new_hyp_sent = hypotheses[prev_hyp_id] + [hyp_word]\n",
        "                if hyp_word == \"</s>\":\n",
        "                    completed_hypotheses.append(\n",
        "                        Hypothesis(value=new_hyp_sent[1:-1], score=cand_new_hyp_score)\n",
        "                    )\n",
        "                else:\n",
        "                    new_hypotheses.append(new_hyp_sent)\n",
        "                    live_hyp_ids.append(prev_hyp_id)\n",
        "                    new_hyp_scores.append(cand_new_hyp_score)\n",
        "\n",
        "            if len(completed_hypotheses) == beam_size:\n",
        "                break\n",
        "\n",
        "            live_hyp_ids = torch.tensor(\n",
        "                live_hyp_ids, dtype=torch.long, device=self.device\n",
        "            )\n",
        "            h_tm1 = (h_t[live_hyp_ids], cell_t[live_hyp_ids])\n",
        "            att_tm1 = att_t[live_hyp_ids]\n",
        "\n",
        "            hypotheses = new_hypotheses\n",
        "            hyp_scores = torch.tensor(\n",
        "                new_hyp_scores, dtype=torch.float, device=self.device\n",
        "            )\n",
        "\n",
        "        if len(completed_hypotheses) == 0:\n",
        "            completed_hypotheses.append(\n",
        "                Hypothesis(value=hypotheses[0][1:], score=hyp_scores[0].item())\n",
        "            )\n",
        "\n",
        "        completed_hypotheses.sort(key=lambda hyp: hyp.score, reverse=True)\n",
        "\n",
        "        return completed_hypotheses\n",
        "\n",
        "    @property\n",
        "    def device(self) -> torch.device:\n",
        "        \"\"\"Determine which device to place the Tensors upon, CPU or GPU.\"\"\"\n",
        "        return self.model_embeddings.source.weight.device\n",
        "\n",
        "    @staticmethod\n",
        "    def load(model_path: str):\n",
        "        \"\"\"Load the model from a file.\n",
        "        @param model_path: path to model\n",
        "        \"\"\"\n",
        "        params = torch.load(model_path, map_location=lambda storage, loc: storage)\n",
        "        args = params[\"args\"]\n",
        "        model = NMT(vocab=params[\"vocab\"], **args)\n",
        "        model.load_state_dict(params[\"state_dict\"])\n",
        "\n",
        "        return model\n",
        "\n",
        "    def save(self, path: str):\n",
        "        \"\"\"Save the model to a file.\n",
        "        @param path: path to the model\n",
        "        \"\"\"\n",
        "        print(\"save model parameters to [%s]\" % path, file=sys.stderr)\n",
        "\n",
        "        params = {\n",
        "            \"args\": dict(\n",
        "                embed_size=self.model_embeddings.embed_size,\n",
        "                hidden_size=self.hidden_size,\n",
        "                dropout_rate=self.dropout_rate,\n",
        "            ),\n",
        "            \"vocab\": self.vocab,\n",
        "            \"state_dict\": self.state_dict(),\n",
        "        }\n",
        "\n",
        "        torch.save(params, path)"
      ],
      "metadata": {
        "id": "LXiTpsmFpYKV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HMjoXSg7qDz3",
        "outputId": "f31fcd1e-5a65-450e-b5ce-f21d2c9689ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.99\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install docopt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CdR0CDwPrzC6",
        "outputId": "70448d16-e8c1-4705-af46-0c7f06e8a06c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting docopt\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: docopt\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13721 sha256=112aa97ea290ccee3bfb2c41b67f6ee82ca004cee0d75d7f52b1ae9c0010c749\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "Successfully built docopt\n",
            "Installing collected packages: docopt\n",
            "Successfully installed docopt-0.6.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from typing import List\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import nltk\n",
        "import sentencepiece as spm\n",
        "from collections import Counter\n",
        "from docopt import docopt\n",
        "from itertools import chain\n",
        "import json\n",
        "#from utils import read_corpus, pad_sents\n"
      ],
      "metadata": {
        "id": "M8jhw-oBp1Bm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download(\"punkt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r5EGiceDp-h4",
        "outputId": "f582e84d-28d0-4fa2-d6b9-f6d5d77f3b4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def pad_sents(sents, pad_token):\n",
        "    \"\"\"Pad list of sentences according to the longest sentence in the batch.\n",
        "        The paddings should be at the end of each sentence.\n",
        "    @param sents (list[list[str]]): list of sentences, where each sentence\n",
        "                                    is represented as a list of words\n",
        "    @param pad_token (str): padding token\n",
        "    @returns sents_padded (list[list[str]]): list of sentences where sentences shorter\n",
        "        than the max length sentence are padded out with the pad_token, such that\n",
        "        each sentences in the batch now has equal length.\n",
        "    \"\"\"\n",
        "    sents_padded = []\n",
        "\n",
        "    # YOUR CODE HERE (~6 Lines)\n",
        "    sentence_lengths = [len(s) for s in sents]\n",
        "    max_length = max(sentence_lengths)\n",
        "    for s in sents:\n",
        "        s += [pad_token] * (max_length - len(s))\n",
        "        sents_padded.append(s)\n",
        "    # END YOUR CODE\n",
        "\n",
        "    return sents_padded"
      ],
      "metadata": {
        "id": "YEIEmt-6rY3y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_corpus(file_path, source, vocab_size=2500):\n",
        "    \"\"\"Read file, where each sentence is dilineated by a `\\n`.\n",
        "    @param file_path (str): path to file containing corpus\n",
        "    @param source (str): \"tgt\" or \"src\" indicating whether text\n",
        "        is of the source language or target language\n",
        "    @param vocab_size (int): number of unique subwords in\n",
        "        vocabulary when reading and tokenizing\n",
        "    \"\"\"\n",
        "    data = []\n",
        "    sp = spm.SentencePieceProcessor()\n",
        "    sp.load(\"{}.model\".format(source))\n",
        "\n",
        "    with open(file_path, \"r\", encoding=\"utf8\") as f:\n",
        "        for line in f:\n",
        "            subword_tokens = sp.encode_as_pieces(line)\n",
        "            # only append <s> and </s> to the target sentence\n",
        "            if source == \"tgt\":\n",
        "                subword_tokens = [\"<s>\"] + subword_tokens + [\"</s>\"]\n",
        "            data.append(subword_tokens)\n",
        "\n",
        "    return data"
      ],
      "metadata": {
        "id": "uaCiGwcPrbmj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def autograder_read_corpus(file_path, source):\n",
        "    \"\"\"Read file, where each sentence is dilineated by a `\\n`.\n",
        "    @param file_path (str): path to file containing corpus\n",
        "    @param source (str): \"tgt\" or \"src\" indicating whether text\n",
        "        is of the source language or target language\n",
        "    \"\"\"\n",
        "    data = []\n",
        "    for line in open(file_path):\n",
        "        sent = nltk.word_tokenize(line)\n",
        "        # only append <s> and </s> to the target sentence\n",
        "        if source == \"tgt\":\n",
        "            sent = [\"<s>\"] + sent + [\"</s>\"]\n",
        "        data.append(sent)\n",
        "\n",
        "    return data"
      ],
      "metadata": {
        "id": "D8iwi-kNrdpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def batch_iter(data, batch_size, shuffle=False):\n",
        "    \"\"\"Yield batches of source and target sentences reverse sorted by length (largest to smallest).\n",
        "    @param data (list of (src_sent, tgt_sent)): list of tuples containing source and target sentence\n",
        "    @param batch_size (int): batch size\n",
        "    @param shuffle (boolean): whether to randomly shuffle the dataset\n",
        "    \"\"\"\n",
        "    batch_num = math.ceil(len(data) / batch_size)\n",
        "    index_array = list(range(len(data)))\n",
        "\n",
        "    if shuffle:\n",
        "        np.random.shuffle(index_array)\n",
        "\n",
        "    for i in range(batch_num):\n",
        "        indices = index_array[i * batch_size : (i + 1) * batch_size]\n",
        "        examples = [data[idx] for idx in indices]\n",
        "\n",
        "        examples = sorted(examples, key=lambda e: len(e[0]), reverse=True)\n",
        "        src_sents = [e[0] for e in examples]\n",
        "        tgt_sents = [e[1] for e in examples]\n",
        "\n",
        "        yield src_sents, tgt_sents"
      ],
      "metadata": {
        "id": "DIVg7x3LrgXD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VocabEntry(object):\n",
        "    \"\"\"Vocabulary Entry, i.e. structure containing either\n",
        "    src or tgt language terms.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, word2id=None):\n",
        "        \"\"\"Init VocabEntry Instance.\n",
        "        @param word2id (dict): dictionary mapping words 2 indices\n",
        "        \"\"\"\n",
        "        if word2id:\n",
        "            self.word2id = word2id\n",
        "        else:\n",
        "            self.word2id = dict()\n",
        "            self.word2id[\"<pad>\"] = 0  # Pad Token\n",
        "            self.word2id[\"<s>\"] = 1  # Start Token\n",
        "            self.word2id[\"</s>\"] = 2  # End Token\n",
        "            self.word2id[\"<unk>\"] = 3  # Unknown Token\n",
        "        self.unk_id = self.word2id[\"<unk>\"]\n",
        "        self.id2word = {v: k for k, v in self.word2id.items()}\n",
        "\n",
        "    def __getitem__(self, word):\n",
        "        \"\"\"Retrieve word's index. Return the index for the unk\n",
        "        token if the word is out of vocabulary.\n",
        "        @param word (str): word to look up.\n",
        "        @returns index (int): index of word\n",
        "        \"\"\"\n",
        "        return self.word2id.get(word, self.unk_id)\n",
        "\n",
        "    def __contains__(self, word):\n",
        "        \"\"\"Check if word is captured by VocabEntry.\n",
        "        @param word (str): word to look up\n",
        "        @returns contains (bool): whether word is contained\n",
        "        \"\"\"\n",
        "        return word in self.word2id\n",
        "\n",
        "    def __setitem__(self, key, value):\n",
        "        \"\"\"Raise error, if one tries to edit the VocabEntry.\"\"\"\n",
        "        raise ValueError(\"vocabulary is readonly\")\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Compute number of words in VocabEntry.\n",
        "        @returns len (int): number of words in VocabEntry\n",
        "        \"\"\"\n",
        "        return len(self.word2id)\n",
        "\n",
        "    def __repr__(self):\n",
        "        \"\"\"Representation of VocabEntry to be used\n",
        "        when printing the object.\n",
        "        \"\"\"\n",
        "        return \"Vocabulary[size=%d]\" % len(self)\n",
        "\n",
        "    def id2word(self, wid):\n",
        "        \"\"\"Return mapping of index to word.\n",
        "        @param wid (int): word index\n",
        "        @returns word (str): word corresponding to index\n",
        "        \"\"\"\n",
        "        return self.id2word[wid]\n",
        "\n",
        "    def add(self, word):\n",
        "        \"\"\"Add word to VocabEntry, if it is previously unseen.\n",
        "        @param word (str): word to add to VocabEntry\n",
        "        @return index (int): index that the word has been assigned\n",
        "        \"\"\"\n",
        "        if word not in self:\n",
        "            wid = self.word2id[word] = len(self)\n",
        "            self.id2word[wid] = word\n",
        "            return wid\n",
        "        else:\n",
        "            return self[word]\n",
        "\n",
        "    def words2indices(self, sents):\n",
        "        \"\"\"Convert list of words or list of sentences of words\n",
        "        into list or list of list of indices.\n",
        "        @param sents (list[str] or list[list[str]]): sentence(s) in words\n",
        "        @return word_ids (list[int] or list[list[int]]): sentence(s) in indices\n",
        "        \"\"\"\n",
        "        if type(sents[0]) == list:\n",
        "            return [[self[w] for w in s] for s in sents]\n",
        "        else:\n",
        "            return [self[w] for w in sents]\n",
        "\n",
        "    def indices2words(self, word_ids):\n",
        "        \"\"\"Convert list of indices into words.\n",
        "        @param word_ids (list[int]): list of word ids\n",
        "        @return sents (list[str]): list of words\n",
        "        \"\"\"\n",
        "        return [self.id2word[w_id] for w_id in word_ids]\n",
        "\n",
        "    def to_input_tensor(\n",
        "        self, sents: List[List[str]], device: torch.device\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"Convert list of sentences (words) into tensor with necessary padding for\n",
        "        shorter sentences.\n",
        "\n",
        "        @param sents (List[List[str]]): list of sentences (words)\n",
        "        @param device: device on which to load the tesnor, i.e. CPU or GPU\n",
        "\n",
        "        @returns sents_var: tensor of (max_sentence_length, batch_size)\n",
        "        \"\"\"\n",
        "        word_ids = self.words2indices(sents)\n",
        "        sents_t = pad_sents(word_ids, self[\"<pad>\"])\n",
        "        sents_var = torch.tensor(sents_t, dtype=torch.long, device=device)\n",
        "        return torch.t(sents_var)\n",
        "\n",
        "    @staticmethod\n",
        "    def from_corpus(corpus, size, freq_cutoff=2):\n",
        "        \"\"\"Given a corpus construct a Vocab Entry.\n",
        "        @param corpus (list[str]): corpus of text produced by read_corpus function\n",
        "        @param size (int): # of words in vocabulary\n",
        "        @param freq_cutoff (int): if word occurs n < freq_cutoff times, drop the word\n",
        "        @returns vocab_entry (VocabEntry): VocabEntry instance produced from provided corpus\n",
        "        \"\"\"\n",
        "        vocab_entry = VocabEntry()\n",
        "        word_freq = Counter(chain(*corpus))\n",
        "        valid_words = [w for w, v in word_freq.items() if v >= freq_cutoff]\n",
        "        print(\n",
        "            \"number of word types: {}, number of word types w/ frequency >= {}: {}\".format(\n",
        "                len(word_freq), freq_cutoff, len(valid_words)\n",
        "            )\n",
        "        )\n",
        "        top_k_words = sorted(valid_words, key=lambda w: word_freq[w], reverse=True)[\n",
        "            :size\n",
        "        ]\n",
        "        for word in top_k_words:\n",
        "            vocab_entry.add(word)\n",
        "        return vocab_entry\n",
        "\n",
        "    @staticmethod\n",
        "    def from_subword_list(subword_list):\n",
        "        vocab_entry = VocabEntry()\n",
        "        for subword in subword_list:\n",
        "            vocab_entry.add(subword)\n",
        "        return vocab_entry\n",
        "\n",
        "\n",
        "class Vocab(object):\n",
        "    \"\"\"Vocab encapsulating src and target languages.\"\"\"\n",
        "\n",
        "    def __init__(self, src_vocab: VocabEntry, tgt_vocab: VocabEntry):\n",
        "        \"\"\"Init Vocab.\n",
        "        @param src_vocab (VocabEntry): VocabEntry for source language\n",
        "        @param tgt_vocab (VocabEntry): VocabEntry for target language\n",
        "        \"\"\"\n",
        "        self.src = src_vocab\n",
        "        self.tgt = tgt_vocab\n",
        "\n",
        "    @staticmethod\n",
        "    def build(src_sents, tgt_sents) -> \"Vocab\":\n",
        "        \"\"\"Build Vocabulary.\n",
        "        @param src_sents (list[str]): Source subwords provided by SentencePiece\n",
        "        @param tgt_sents (list[str]): Target subwords provided by SentencePiece\n",
        "        \"\"\"\n",
        "\n",
        "        print(\"initialize source vocabulary ..\")\n",
        "        src = VocabEntry.from_subword_list(src_sents)\n",
        "\n",
        "        print(\"initialize target vocabulary ..\")\n",
        "        tgt = VocabEntry.from_subword_list(tgt_sents)\n",
        "\n",
        "        return Vocab(src, tgt)\n",
        "\n",
        "    def save(self, file_path):\n",
        "        \"\"\"Save Vocab to file as JSON dump.\n",
        "        @param file_path (str): file path to vocab file\n",
        "        \"\"\"\n",
        "        with open(file_path, \"w\") as f:\n",
        "            json.dump(\n",
        "                dict(src_word2id=self.src.word2id, tgt_word2id=self.tgt.word2id),\n",
        "                f,\n",
        "                indent=2,\n",
        "            )\n",
        "\n",
        "    @staticmethod\n",
        "    def load(file_path):\n",
        "        \"\"\"Load vocabulary from JSON dump.\n",
        "        @param file_path (str): file path to vocab file\n",
        "        @returns Vocab object loaded from JSON dump\n",
        "        \"\"\"\n",
        "        entry = json.load(open(file_path, \"r\"))\n",
        "        src_word2id = entry[\"src_word2id\"]\n",
        "        tgt_word2id = entry[\"tgt_word2id\"]\n",
        "\n",
        "        return Vocab(VocabEntry(src_word2id), VocabEntry(tgt_word2id))\n",
        "\n",
        "    def __repr__(self):\n",
        "        \"\"\"Representation of Vocab to be used\n",
        "        when printing the object.\n",
        "        \"\"\"\n",
        "        return \"Vocab(source %d words, target %d words)\" % (\n",
        "            len(self.src),\n",
        "            len(self.tgt),\n",
        "        )\n",
        "\n"
      ],
      "metadata": {
        "id": "gBMG2vfyri0A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_vocab_list(file_path, source, vocab_size):\n",
        "    \"\"\"Use SentencePiece to tokenize and acquire list of unique subwords.\n",
        "    @param file_path (str): file path to corpus\n",
        "    @param source (str): tgt or src\n",
        "    @param vocab_size: desired vocabulary size\n",
        "    \"\"\"\n",
        "    spm.SentencePieceTrainer.train(\n",
        "        input=file_path, model_prefix=source, vocab_size=vocab_size\n",
        "    )  # train the spm model\n",
        "    sp = (\n",
        "        spm.SentencePieceProcessor()\n",
        "    )  # create an instance; this saves .model and .vocab files\n",
        "    sp.load(\"{}.model\".format(source))  # loads tgt.model or src.model\n",
        "    sp_list = [\n",
        "        sp.id_to_piece(piece_id) for piece_id in range(sp.get_piece_size())\n",
        "    ]  # this is the list of subwords\n",
        "    return sp_list"
      ],
      "metadata": {
        "id": "KuW9DzFrr9AD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Hardcode your desired argument values\n",
        "    args = {\n",
        "        \"--train-src\": \"/content/drive/MyDrive/ISR Spring'23/src.vocab\",\n",
        "        \"--train-tgt\": \"/content/drive/MyDrive/ISR Spring'23/tgt.vocab\",\n",
        "        \"VOCAB_FILE\": \"/content/drive/MyDrive/ISR Spring'23/vocab_file.json\"\n",
        "    }\n",
        "\n",
        "    print(\"read in source sentences: %s\" % args[\"--train-src\"])\n",
        "    print(\"read in target sentences: %s\" % args[\"--train-tgt\"])\n",
        "\n",
        "    src_sents = get_vocab_list(args[\"--train-src\"], source=\"src\", vocab_size=14128)\n",
        "    tgt_sents = get_vocab_list(args[\"--train-tgt\"], source=\"tgt\", vocab_size=6748)\n",
        "    vocab = Vocab.build(src_sents, tgt_sents)\n",
        "    print(\n",
        "        \"generated vocabulary, source %d words, target %d words\"\n",
        "        % (len(src_sents), len(tgt_sents))\n",
        "    )\n",
        "\n",
        "    vocab.save(args[\"VOCAB_FILE\"])\n",
        "    print(\"vocabulary saved to %s\" % args[\"VOCAB_FILE\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_GEVrMpb7PUM",
        "outputId": "d8007faa-a14f-4f17-ee2f-256acbe0c3d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "read in source sentences: /content/drive/MyDrive/ISR Spring'23/src.vocab\n",
            "read in target sentences: /content/drive/MyDrive/ISR Spring'23/tgt.vocab\n",
            "initialize source vocabulary ..\n",
            "initialize target vocabulary ..\n",
            "generated vocabulary, source 14128 words, target 6748 words\n",
            "vocabulary saved to /content/drive/MyDrive/ISR Spring'23/vocab_file.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sacrebleu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OgKG5kyP8cnS",
        "outputId": "b05d28bd-6967-457c-fa0b-a8dd9f5277b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sacrebleu\n",
            "  Downloading sacrebleu-2.3.1-py3-none-any.whl (118 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.9/118.9 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (2022.10.31)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (4.9.2)\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.8.10)\n",
            "Collecting portalocker\n",
            "  Downloading portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (1.22.4)\n",
            "Installing collected packages: portalocker, colorama, sacrebleu\n",
            "Successfully installed colorama-0.4.6 portalocker-2.7.0 sacrebleu-2.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import sys\n",
        "import pickle\n",
        "import time\n",
        "\n",
        "\n",
        "from docopt import docopt\n",
        "\n",
        "# from nltk.translate.bleu_score import corpus_bleu, sentence_bleu, SmoothingFunction\n",
        "import sacrebleu\n",
        "#from nmt_model import Hypothesis, NMT\n",
        "import numpy as np\n",
        "from typing import List, Tuple, Dict, Set, Union\n",
        "from tqdm import tqdm\n",
        "#from utils import read_corpus, batch_iter\n",
        "#from vocab import Vocab, VocabEntry\n"
      ],
      "metadata": {
        "id": "W05aePhts1Vc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_ppl(model, dev_data, batch_size=32):\n",
        "    \"\"\"Evaluate perplexity on dev sentences\n",
        "    @param model (NMT): NMT Model\n",
        "    @param dev_data (list of (src_sent, tgt_sent)): list of tuples containing source and target sentence\n",
        "    @param batch_size (batch size)\n",
        "    @returns ppl (perplixty on dev sentences)\n",
        "    \"\"\"\n",
        "    was_training = model.training\n",
        "    model.eval()\n",
        "\n",
        "    cum_loss = 0.0\n",
        "    cum_tgt_words = 0.0\n",
        "\n",
        "    # no_grad() signals backend to throw away all gradients\n",
        "    with torch.no_grad():\n",
        "        for src_sents, tgt_sents in batch_iter(dev_data, batch_size):\n",
        "            loss = -model(src_sents, tgt_sents).sum()\n",
        "\n",
        "            cum_loss += loss.item()\n",
        "            tgt_word_num_to_predict = sum(\n",
        "                len(s[1:]) for s in tgt_sents\n",
        "            )  # omitting leading `<s>`\n",
        "            cum_tgt_words += tgt_word_num_to_predict\n",
        "\n",
        "        ppl = np.exp(cum_loss / cum_tgt_words)\n",
        "\n",
        "    if was_training:\n",
        "        model.train()\n",
        "\n",
        "    return ppl"
      ],
      "metadata": {
        "id": "JJR4deC-8bbb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_corpus_level_bleu_score(\n",
        "    references: List[List[str]], hypotheses: List[Hypothesis]\n",
        ") -> float:\n",
        "    \"\"\"Given decoding results and reference sentences, compute corpus-level BLEU score.\n",
        "    @param references (List[List[str]]): a list of gold-standard reference target sentences\n",
        "    @param hypotheses (List[Hypothesis]): a list of hypotheses, one for each reference\n",
        "    @returns bleu_score: corpus-level BLEU score\n",
        "    \"\"\"\n",
        "    # remove the start and end tokens\n",
        "    if references[0][0] == \"<s>\":\n",
        "        references = [ref[1:-1] for ref in references]\n",
        "\n",
        "    # detokenize the subword pieces to get full sentences\n",
        "    detokened_refs = [\"\".join(pieces).replace(\"▁\", \" \") for pieces in references]\n",
        "    detokened_hyps = [\"\".join(hyp.value).replace(\"▁\", \" \") for hyp in hypotheses]\n",
        "\n",
        "    # sacreBLEU can take multiple references (golden example per sentence) but we only feed it one\n",
        "    bleu = sacrebleu.corpus_bleu(detokened_hyps, [detokened_refs])\n",
        "\n",
        "    return bleu.score"
      ],
      "metadata": {
        "id": "hvI665j-9F1R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(args: Dict):\n",
        "    \"\"\"Train the NMT Model.\n",
        "    @param args (Dict): args from cmd line\n",
        "    \"\"\"\n",
        "    train_data_src = read_corpus(args[\"--train-src\"], source=\"src\", vocab_size=21000)\n",
        "    train_data_tgt = read_corpus(args[\"--train-tgt\"], source=\"tgt\", vocab_size=8000)\n",
        "\n",
        "    dev_data_src = read_corpus(args[\"--dev-src\"], source=\"src\", vocab_size=3000)\n",
        "    dev_data_tgt = read_corpus(args[\"--dev-tgt\"], source=\"tgt\", vocab_size=2000)\n",
        "\n",
        "    train_data = list(zip(train_data_src, train_data_tgt))\n",
        "    dev_data = list(zip(dev_data_src, dev_data_tgt))\n",
        "\n",
        "    train_batch_size = int(args[\"--batch-size\"])\n",
        "    clip_grad = float(args[\"--clip-grad\"])\n",
        "    valid_niter = int(args[\"--valid-niter\"])\n",
        "    log_every = int(args[\"--log-every\"])\n",
        "    model_save_path = args[\"--save-to\"]\n",
        "\n",
        "    vocab = Vocab.load(args[\"--vocab\"])\n",
        "\n",
        "    # model = NMT(embed_size=int(args['--embed-size']),\n",
        "    #             hidden_size=int(args['--hidden-size']),\n",
        "    #             dropout_rate=float(args['--dropout']),\n",
        "    #             vocab=vocab)\n",
        "\n",
        "    model = NMT(\n",
        "        embed_size=1024,\n",
        "        hidden_size=1024,\n",
        "        dropout_rate=float(args[\"--dropout\"]),\n",
        "        vocab=vocab,\n",
        "    )\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    uniform_init = float(args[\"--uniform-init\"])\n",
        "    if np.abs(uniform_init) > 0.0:\n",
        "        print(\n",
        "            \"uniformly initialize parameters [-%f, +%f]\" % (uniform_init, uniform_init),\n",
        "            file=sys.stderr,\n",
        "        )\n",
        "        for p in model.parameters():\n",
        "            p.data.uniform_(-uniform_init, uniform_init)\n",
        "\n",
        "    vocab_mask = torch.ones(len(vocab.tgt))\n",
        "    vocab_mask[vocab.tgt[\"<pad>\"]] = 0\n",
        "\n",
        "    device = torch.device(\"cuda:0\" if args[\"--cuda\"] else \"cpu\")\n",
        "    print(\"use device: %s\" % device, file=sys.stderr)\n",
        "\n",
        "    model = model.to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=float(args[\"--lr\"]))\n",
        "\n",
        "    num_trial = 0\n",
        "    train_iter = (\n",
        "        patience\n",
        "    ) = cum_loss = report_loss = cum_tgt_words = report_tgt_words = 0\n",
        "    cum_examples = report_examples = epoch = valid_num = 0\n",
        "    hist_valid_scores = []\n",
        "    train_time = begin_time = time.time()\n",
        "    print(\"begin Maximum Likelihood training\")\n",
        "\n",
        "    while True:\n",
        "        epoch += 1\n",
        "\n",
        "        for src_sents, tgt_sents in batch_iter(\n",
        "            train_data, batch_size=train_batch_size, shuffle=True\n",
        "        ):\n",
        "            train_iter += 1\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            batch_size = len(src_sents)\n",
        "\n",
        "            example_losses = -model(src_sents, tgt_sents)  # (batch_size,)\n",
        "            batch_loss = example_losses.sum()\n",
        "            loss = batch_loss / batch_size\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            # clip gradient\n",
        "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad)\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            batch_losses_val = batch_loss.item()\n",
        "            report_loss += batch_losses_val\n",
        "            cum_loss += batch_losses_val\n",
        "\n",
        "            tgt_words_num_to_predict = sum(\n",
        "                len(s[1:]) for s in tgt_sents\n",
        "            )  # omitting leading `<s>`\n",
        "            report_tgt_words += tgt_words_num_to_predict\n",
        "            cum_tgt_words += tgt_words_num_to_predict\n",
        "            report_examples += batch_size\n",
        "            cum_examples += batch_size\n",
        "\n",
        "            if train_iter % log_every == 0:\n",
        "                print(\n",
        "                    \"epoch %d, iter %d, avg. loss %.2f, avg. ppl %.2f \"\n",
        "                    \"cum. examples %d, speed %.2f words/sec, time elapsed %.2f sec\"\n",
        "                    % (\n",
        "                        epoch,\n",
        "                        train_iter,\n",
        "                        report_loss / report_examples,\n",
        "                        math.exp(report_loss / report_tgt_words),\n",
        "                        cum_examples,\n",
        "                        report_tgt_words / (time.time() - train_time),\n",
        "                        time.time() - begin_time,\n",
        "                    ),\n",
        "                    file=sys.stderr,\n",
        "                )\n",
        "\n",
        "                train_time = time.time()\n",
        "                report_loss = report_tgt_words = report_examples = 0.0\n",
        "\n",
        "            # perform validation\n",
        "            if train_iter % valid_niter == 0:\n",
        "                print(\n",
        "                    \"epoch %d, iter %d, cum. loss %.2f, cum. ppl %.2f cum. examples %d\"\n",
        "                    % (\n",
        "                        epoch,\n",
        "                        train_iter,\n",
        "                        cum_loss / cum_examples,\n",
        "                        np.exp(cum_loss / cum_tgt_words),\n",
        "                        cum_examples,\n",
        "                    ),\n",
        "                    file=sys.stderr,\n",
        "                )\n",
        "\n",
        "                cum_loss = cum_examples = cum_tgt_words = 0.0\n",
        "                valid_num += 1\n",
        "\n",
        "                print(\"begin validation ...\", file=sys.stderr)\n",
        "\n",
        "                # compute dev. ppl and bleu\n",
        "                dev_ppl = evaluate_ppl(\n",
        "                    model, dev_data, batch_size=128\n",
        "                )  # dev batch size can be a bit larger\n",
        "                valid_metric = -dev_ppl\n",
        "\n",
        "                print(\n",
        "                    \"validation: iter %d, dev. ppl %f\" % (train_iter, dev_ppl),\n",
        "                    file=sys.stderr,\n",
        "                )\n",
        "\n",
        "                is_better = len(hist_valid_scores) == 0 or valid_metric > max(\n",
        "                    hist_valid_scores\n",
        "                )\n",
        "                hist_valid_scores.append(valid_metric)\n",
        "\n",
        "                if is_better:\n",
        "                    patience = 0\n",
        "                    print(\n",
        "                        \"save currently the best model to [%s]\" % model_save_path,\n",
        "                        file=sys.stderr,\n",
        "                    )\n",
        "                    model.save(model_save_path)\n",
        "\n",
        "                    # also save the optimizers' state\n",
        "                    torch.save(optimizer.state_dict(), model_save_path + \".optim\")\n",
        "                elif patience < int(args[\"--patience\"]):\n",
        "                    patience += 1\n",
        "                    print(\"hit patience %d\" % patience, file=sys.stderr)\n",
        "\n",
        "                    if patience == int(args[\"--patience\"]):\n",
        "                        num_trial += 1\n",
        "                        print(\"hit #%d trial\" % num_trial, file=sys.stderr)\n",
        "                        if num_trial == int(args[\"--max-num-trial\"]):\n",
        "                            print(\"early stop!\", file=sys.stderr)\n",
        "                            exit(0)\n",
        "\n",
        "                        # decay lr, and restore from previously best checkpoint\n",
        "                        lr = optimizer.param_groups[0][\"lr\"] * float(args[\"--lr-decay\"])\n",
        "                        print(\n",
        "                            \"load previously best model and decay learning rate to %f\"\n",
        "                            % lr,\n",
        "                            file=sys.stderr,\n",
        "                        )\n",
        "\n",
        "                        # load model\n",
        "                        params = torch.load(\n",
        "                            model_save_path, map_location=lambda storage, loc: storage\n",
        "                        )\n",
        "                        model.load_state_dict(params[\"state_dict\"])\n",
        "                        model = model.to(device)\n",
        "\n",
        "                        print(\"restore parameters of the optimizers\", file=sys.stderr)\n",
        "                        optimizer.load_state_dict(\n",
        "                            torch.load(model_save_path + \".optim\")\n",
        "                        )\n",
        "\n",
        "                        # set new lr\n",
        "                        for param_group in optimizer.param_groups:\n",
        "                            param_group[\"lr\"] = lr\n",
        "\n",
        "                        # reset patience\n",
        "                        patience = 0\n",
        "\n",
        "                if epoch == int(args[\"--max-epoch\"]):\n",
        "                    print(\"reached maximum number of epochs!\", file=sys.stderr)\n",
        "                    exit(0)"
      ],
      "metadata": {
        "id": "aG9n8PXD9OEl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def beam_search(\n",
        "    model: NMT,\n",
        "    test_data_src: List[List[str]],\n",
        "    beam_size: int,\n",
        "    max_decoding_time_step: int,\n",
        ") -> List[List[Hypothesis]]:\n",
        "    \"\"\"Run beam search to construct hypotheses for a list of src-language sentences.\n",
        "    @param model (NMT): NMT Model\n",
        "    @param test_data_src (List[List[str]]): List of sentences (words) in source language, from test set.\n",
        "    @param beam_size (int): beam_size (# of hypotheses to hold for a translation at every step)\n",
        "    @param max_decoding_time_step (int): maximum sentence length that Beam search can produce\n",
        "    @returns hypotheses (List[List[Hypothesis]]): List of Hypothesis translations for every source sentence.\n",
        "    \"\"\"\n",
        "    was_training = model.training\n",
        "    model.eval()\n",
        "\n",
        "    hypotheses = []\n",
        "    with torch.no_grad():\n",
        "        for src_sent in tqdm(test_data_src, desc=\"Decoding\", file=sys.stdout):\n",
        "            example_hyps = model.beam_search(\n",
        "                src_sent,\n",
        "                beam_size=beam_size,\n",
        "                max_decoding_time_step=max_decoding_time_step,\n",
        "            )\n",
        "\n",
        "            hypotheses.append(example_hyps)\n",
        "\n",
        "    if was_training:\n",
        "        model.train(was_training)\n",
        "\n",
        "    return hypotheses"
      ],
      "metadata": {
        "id": "XWv08wgkAH33"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decode(args: Dict[str, str]):\n",
        "    \"\"\"Performs decoding on a test set, and save the best-scoring decoding results.\n",
        "    If the target gold-standard sentences are given, the function also computes\n",
        "    corpus-level BLEU score.\n",
        "    @param args (Dict): args from cmd line\n",
        "    \"\"\"\n",
        "\n",
        "    print(\n",
        "        \"load test source sentences from [{}]\".format(args[\"TEST_SOURCE_FILE\"]),\n",
        "        file=sys.stderr,\n",
        "    )\n",
        "    test_data_src = read_corpus(args[\"TEST_SOURCE_FILE\"], source=\"src\", vocab_size=3000)\n",
        "    if args[\"TEST_TARGET_FILE\"]:\n",
        "        print(\n",
        "            \"load test target sentences from [{}]\".format(args[\"TEST_TARGET_FILE\"]),\n",
        "            file=sys.stderr,\n",
        "        )\n",
        "        test_data_tgt = read_corpus(\n",
        "            args[\"TEST_TARGET_FILE\"], source=\"tgt\", vocab_size=2000\n",
        "        )\n",
        "\n",
        "    print(\"load model from {}\".format(args[\"MODEL_PATH\"]), file=sys.stderr)\n",
        "    model = NMT.load(args[\"MODEL_PATH\"])\n",
        "\n",
        "    if args[\"--cuda\"]:\n",
        "        model = model.to(torch.device(\"cuda:0\"))\n",
        "\n",
        "    hypotheses = beam_search(\n",
        "        model,\n",
        "        test_data_src,\n",
        "        #  beam_size=int(args['--beam-size']),\n",
        "        beam_size=10,\n",
        "        max_decoding_time_step=int(args[\"--max-decoding-time-step\"]),\n",
        "    )\n",
        "\n",
        "    if args[\"TEST_TARGET_FILE\"]:\n",
        "        top_hypotheses = [hyps[0] for hyps in hypotheses]\n",
        "        bleu_score = compute_corpus_level_bleu_score(test_data_tgt, top_hypotheses)\n",
        "        print(\"Corpus BLEU: {}\".format(bleu_score), file=sys.stderr)\n",
        "\n",
        "    with open(args[\"OUTPUT_FILE\"], \"w\") as f:\n",
        "        for src_sent, hyps in zip(test_data_src, hypotheses):\n",
        "            top_hyp = hyps[0]\n",
        "            hyp_sent = \"\".join(top_hyp.value).replace(\"▁\", \" \")\n",
        "            f.write(hyp_sent + \"\\n\")"
      ],
      "metadata": {
        "id": "QMjhUZ759ZUu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    \"\"\"Main func.\"\"\"\n",
        "    args = docopt(__doc__)\n",
        "\n",
        "    # Check pytorch version\n",
        "    assert (\n",
        "        torch.__version__ >= \"1.0.0\"\n",
        "    ), \"Please update your installation of PyTorch. You have {} and you should have version 1.0.0\".format(\n",
        "        torch.__version__\n",
        "    )\n",
        "\n",
        "    # seed the random number generators\n",
        "    seed = int(args[\"--seed\"])\n",
        "    torch.manual_seed(seed)\n",
        "    if args[\"--cuda\"]:\n",
        "        torch.cuda.manual_seed(seed)\n",
        "    np.random.seed(seed * 13 // 7)\n",
        "\n",
        "    if args[\"train\"]:\n",
        "        train(args)\n",
        "    elif args[\"decode\"]:\n",
        "        decode(args)\n",
        "    else:\n",
        "        raise RuntimeError(\"invalid run mode\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        },
        "id": "4oy7bC_q_5Ni",
        "outputId": "22ca8850-db9b-4728-f745-243c8f6035a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "DocoptLanguageError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mDocoptLanguageError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-d16e68968c1c>\u001b[0m in \u001b[0;36m<cell line: 27>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-32-d16e68968c1c>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;34m\"\"\"Main func.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdocopt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Check pytorch version\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/docopt.py\u001b[0m in \u001b[0;36mdocopt\u001b[0;34m(doc, argv, help, version, options_first)\u001b[0m\n\u001b[1;32m    556\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0margv\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m         \u001b[0margv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 558\u001b[0;31m     \u001b[0mDocoptExit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprintable_usage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    559\u001b[0m     \u001b[0moptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_defaults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m     \u001b[0mpattern\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_pattern\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformal_usage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDocoptExit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/docopt.py\u001b[0m in \u001b[0;36mprintable_usage\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m    466\u001b[0m     \u001b[0musage_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'([Uu][Ss][Aa][Gg][Ee]:)'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0musage_split\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mDocoptLanguageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\"usage:\" (case-insensitive) not found.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0musage_split\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mDocoptLanguageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'More than one \"usage:\" (case-insensitive).'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mDocoptLanguageError\u001b[0m: \"usage:\" (case-insensitive) not found."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content/drive/MyDrive/ChrEn\n"
      ],
      "metadata": {
        "id": "30XnqBwrAWqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install docopt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7VTTZLSw03Ub",
        "outputId": "947d62cb-bdc1-43b8-ca4b-2b6788c637e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting docopt\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: docopt\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13707 sha256=9e3df16199f1db6d57a88dce9c9161a51a587b7de900f240c48e4e0466f3d61d\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "Successfully built docopt\n",
            "Installing collected packages: docopt\n",
            "Successfully installed docopt-0.6.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VSFJjGg-0_Mr",
        "outputId": "fbb11cc9-aa7b-4477-c5d1-2e93b8effca6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.99\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sacrebleu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tMXykZN63_YV",
        "outputId": "9436b5f5-18d0-4ee1-d264-f21690ad18a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sacrebleu\n",
            "  Downloading sacrebleu-2.3.1-py3-none-any.whl (118 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.9/118.9 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting portalocker (from sacrebleu)\n",
            "  Downloading portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (2022.10.31)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.8.10)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (1.22.4)\n",
            "Collecting colorama (from sacrebleu)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (4.9.2)\n",
            "Installing collected packages: portalocker, colorama, sacrebleu\n",
            "Successfully installed colorama-0.4.6 portalocker-2.7.0 sacrebleu-2.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!bash /content/drive/MyDrive/ChrEn/run.sh train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CpkcRSOG0zG8",
        "outputId": "fd143981-c76d-4648-a8b7-df2cf991d5bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "uniformly initialize parameters [-0.100000, +0.100000]\n",
            "use device: cuda:0\n",
            "begin Maximum Likelihood training\n",
            "epoch 1, iter 10, avg. loss 115.61, avg. ppl 53.48 cum. examples 320, speed 2820.80 words/sec, time elapsed 3.30 sec\n",
            "epoch 1, iter 20, avg. loss 36.12, avg. ppl 3.43 cum. examples 640, speed 3298.13 words/sec, time elapsed 6.14 sec\n",
            "epoch 1, iter 30, avg. loss 16.78, avg. ppl 1.81 cum. examples 960, speed 3512.10 words/sec, time elapsed 8.72 sec\n",
            "epoch 1, iter 40, avg. loss 13.88, avg. ppl 1.57 cum. examples 1280, speed 3555.87 words/sec, time elapsed 11.47 sec\n",
            "epoch 1, iter 50, avg. loss 13.74, avg. ppl 1.58 cum. examples 1600, speed 3726.07 words/sec, time elapsed 14.05 sec\n",
            "epoch 1, iter 60, avg. loss 12.59, avg. ppl 1.55 cum. examples 1920, speed 3756.25 words/sec, time elapsed 16.50 sec\n",
            "epoch 1, iter 70, avg. loss 12.30, avg. ppl 1.51 cum. examples 2240, speed 3490.96 words/sec, time elapsed 19.24 sec\n",
            "epoch 1, iter 80, avg. loss 11.17, avg. ppl 1.47 cum. examples 2560, speed 3546.22 words/sec, time elapsed 21.86 sec\n",
            "epoch 1, iter 90, avg. loss 11.55, avg. ppl 1.51 cum. examples 2880, speed 3410.36 words/sec, time elapsed 24.49 sec\n",
            "epoch 1, iter 100, avg. loss 10.07, avg. ppl 1.41 cum. examples 3200, speed 3854.09 words/sec, time elapsed 26.94 sec\n",
            "epoch 1, iter 110, avg. loss 9.78, avg. ppl 1.41 cum. examples 3520, speed 3952.65 words/sec, time elapsed 29.22 sec\n",
            "epoch 1, iter 120, avg. loss 9.45, avg. ppl 1.41 cum. examples 3840, speed 3887.88 words/sec, time elapsed 31.47 sec\n",
            "epoch 1, iter 130, avg. loss 9.91, avg. ppl 1.40 cum. examples 4160, speed 3812.83 words/sec, time elapsed 33.92 sec\n",
            "epoch 1, iter 140, avg. loss 8.95, avg. ppl 1.37 cum. examples 4480, speed 3933.30 words/sec, time elapsed 36.22 sec\n",
            "epoch 1, iter 150, avg. loss 10.03, avg. ppl 1.41 cum. examples 4800, speed 3873.29 words/sec, time elapsed 38.62 sec\n",
            "epoch 1, iter 160, avg. loss 9.32, avg. ppl 1.40 cum. examples 5120, speed 3602.35 words/sec, time elapsed 41.08 sec\n",
            "epoch 1, iter 170, avg. loss 9.53, avg. ppl 1.39 cum. examples 5440, speed 3390.53 words/sec, time elapsed 43.84 sec\n",
            "epoch 1, iter 180, avg. loss 9.88, avg. ppl 1.39 cum. examples 5760, speed 3801.12 words/sec, time elapsed 46.38 sec\n",
            "epoch 1, iter 190, avg. loss 9.16, avg. ppl 1.36 cum. examples 6080, speed 3006.00 words/sec, time elapsed 49.52 sec\n",
            "epoch 1, iter 200, avg. loss 8.68, avg. ppl 1.34 cum. examples 6400, speed 3467.02 words/sec, time elapsed 52.23 sec\n",
            "epoch 1, iter 200, cum. loss 17.42, cum. ppl 1.82 cum. examples 6400\n",
            "begin validation ...\n",
            "validation: iter 200, dev. ppl 1.334126\n",
            "save currently the best model to [model.bin]\n",
            "save model parameters to [model.bin]\n",
            "epoch 1, iter 210, avg. loss 8.43, avg. ppl 1.37 cum. examples 320, speed 270.86 words/sec, time elapsed 83.89 sec\n",
            "epoch 1, iter 220, avg. loss 8.84, avg. ppl 1.35 cum. examples 640, speed 3912.85 words/sec, time elapsed 86.31 sec\n",
            "epoch 1, iter 230, avg. loss 8.02, avg. ppl 1.33 cum. examples 960, speed 3667.93 words/sec, time elapsed 88.74 sec\n",
            "epoch 1, iter 240, avg. loss 8.54, avg. ppl 1.34 cum. examples 1280, speed 3607.03 words/sec, time elapsed 91.34 sec\n",
            "epoch 1, iter 250, avg. loss 8.67, avg. ppl 1.34 cum. examples 1600, speed 3613.58 words/sec, time elapsed 93.99 sec\n",
            "epoch 1, iter 260, avg. loss 8.52, avg. ppl 1.33 cum. examples 1920, speed 3809.29 words/sec, time elapsed 96.53 sec\n",
            "epoch 1, iter 270, avg. loss 7.06, avg. ppl 1.29 cum. examples 2240, speed 3861.58 words/sec, time elapsed 98.81 sec\n",
            "epoch 1, iter 280, avg. loss 7.44, avg. ppl 1.29 cum. examples 2560, speed 3674.24 words/sec, time elapsed 101.33 sec\n",
            "epoch 1, iter 290, avg. loss 7.38, avg. ppl 1.28 cum. examples 2880, speed 3574.85 words/sec, time elapsed 103.98 sec\n",
            "epoch 1, iter 300, avg. loss 8.62, avg. ppl 1.35 cum. examples 3200, speed 3672.78 words/sec, time elapsed 106.50 sec\n",
            "epoch 1, iter 310, avg. loss 21.71, avg. ppl 2.09 cum. examples 3520, speed 3866.58 words/sec, time elapsed 108.93 sec\n",
            "epoch 1, iter 320, avg. loss 9.94, avg. ppl 1.42 cum. examples 3840, speed 3637.13 words/sec, time elapsed 111.41 sec\n",
            "epoch 1, iter 330, avg. loss 6.70, avg. ppl 1.26 cum. examples 4160, speed 3775.94 words/sec, time elapsed 113.88 sec\n",
            "epoch 1, iter 340, avg. loss 6.08, avg. ppl 1.26 cum. examples 4480, speed 3800.31 words/sec, time elapsed 116.11 sec\n",
            "epoch 1, iter 350, avg. loss 6.12, avg. ppl 1.25 cum. examples 4800, speed 3702.10 words/sec, time elapsed 118.52 sec\n",
            "epoch 1, iter 360, avg. loss 6.38, avg. ppl 1.26 cum. examples 5120, speed 3570.23 words/sec, time elapsed 121.02 sec\n",
            "epoch 1, iter 370, avg. loss 6.17, avg. ppl 1.25 cum. examples 5440, speed 3442.74 words/sec, time elapsed 123.61 sec\n",
            "epoch 1, iter 380, avg. loss 6.80, avg. ppl 1.28 cum. examples 5760, speed 3516.11 words/sec, time elapsed 126.12 sec\n",
            "epoch 1, iter 390, avg. loss 6.12, avg. ppl 1.23 cum. examples 6080, speed 3526.01 words/sec, time elapsed 128.79 sec\n",
            "epoch 1, iter 400, avg. loss 5.75, avg. ppl 1.22 cum. examples 6400, speed 3310.70 words/sec, time elapsed 131.63 sec\n",
            "epoch 1, iter 400, cum. loss 8.16, cum. ppl 1.33 cum. examples 6400\n",
            "begin validation ...\n",
            "validation: iter 400, dev. ppl 1.256969\n",
            "save currently the best model to [model.bin]\n",
            "save model parameters to [model.bin]\n",
            "epoch 1, iter 410, avg. loss 5.84, avg. ppl 1.23 cum. examples 320, speed 301.46 words/sec, time elapsed 161.73 sec\n",
            "epoch 1, iter 420, avg. loss 5.44, avg. ppl 1.20 cum. examples 640, speed 3876.26 words/sec, time elapsed 164.19 sec\n",
            "epoch 1, iter 430, avg. loss 5.53, avg. ppl 1.22 cum. examples 960, speed 3706.04 words/sec, time elapsed 166.62 sec\n",
            "epoch 1, iter 440, avg. loss 4.74, avg. ppl 1.18 cum. examples 1280, speed 3718.06 words/sec, time elapsed 169.14 sec\n",
            "epoch 1, iter 450, avg. loss 6.56, avg. ppl 1.27 cum. examples 1600, speed 3433.00 words/sec, time elapsed 171.74 sec\n",
            "epoch 1, iter 460, avg. loss 4.90, avg. ppl 1.19 cum. examples 1920, speed 3510.74 words/sec, time elapsed 174.35 sec\n",
            "epoch 2, iter 470, avg. loss 5.68, avg. ppl 1.21 cum. examples 2215, speed 3481.60 words/sec, time elapsed 176.90 sec\n",
            "epoch 2, iter 480, avg. loss 4.95, avg. ppl 1.19 cum. examples 2535, speed 3772.22 words/sec, time elapsed 179.37 sec\n",
            "epoch 2, iter 490, avg. loss 4.94, avg. ppl 1.19 cum. examples 2855, speed 3325.33 words/sec, time elapsed 182.06 sec\n",
            "epoch 2, iter 500, avg. loss 4.55, avg. ppl 1.18 cum. examples 3175, speed 3247.26 words/sec, time elapsed 184.79 sec\n",
            "epoch 2, iter 510, avg. loss 3.72, avg. ppl 1.14 cum. examples 3495, speed 3391.07 words/sec, time elapsed 187.49 sec\n",
            "epoch 2, iter 520, avg. loss 4.46, avg. ppl 1.18 cum. examples 3815, speed 3712.37 words/sec, time elapsed 189.84 sec\n",
            "epoch 2, iter 530, avg. loss 4.28, avg. ppl 1.16 cum. examples 4135, speed 3641.90 words/sec, time elapsed 192.45 sec\n",
            "epoch 2, iter 540, avg. loss 3.76, avg. ppl 1.14 cum. examples 4455, speed 3730.23 words/sec, time elapsed 194.93 sec\n",
            "epoch 2, iter 550, avg. loss 3.83, avg. ppl 1.14 cum. examples 4775, speed 3365.04 words/sec, time elapsed 197.64 sec\n",
            "epoch 2, iter 560, avg. loss 3.86, avg. ppl 1.14 cum. examples 5095, speed 3873.66 words/sec, time elapsed 200.07 sec\n",
            "epoch 2, iter 570, avg. loss 4.18, avg. ppl 1.16 cum. examples 5415, speed 3463.60 words/sec, time elapsed 202.71 sec\n",
            "epoch 2, iter 580, avg. loss 3.55, avg. ppl 1.13 cum. examples 5735, speed 3688.82 words/sec, time elapsed 205.15 sec\n",
            "epoch 2, iter 590, avg. loss 3.66, avg. ppl 1.13 cum. examples 6055, speed 3402.48 words/sec, time elapsed 208.02 sec\n",
            "epoch 2, iter 600, avg. loss 3.55, avg. ppl 1.13 cum. examples 6375, speed 3740.42 words/sec, time elapsed 210.59 sec\n",
            "epoch 2, iter 600, cum. loss 4.59, cum. ppl 1.17 cum. examples 6375\n",
            "begin validation ...\n",
            "validation: iter 600, dev. ppl 1.122461\n",
            "save currently the best model to [model.bin]\n",
            "save model parameters to [model.bin]\n",
            "epoch 2, iter 610, avg. loss 4.32, avg. ppl 1.16 cum. examples 320, speed 308.65 words/sec, time elapsed 240.77 sec\n",
            "epoch 2, iter 620, avg. loss 4.10, avg. ppl 1.15 cum. examples 640, speed 3968.97 words/sec, time elapsed 243.16 sec\n",
            "epoch 2, iter 630, avg. loss 3.13, avg. ppl 1.12 cum. examples 960, speed 3616.47 words/sec, time elapsed 245.64 sec\n",
            "epoch 2, iter 640, avg. loss 4.16, avg. ppl 1.15 cum. examples 1280, speed 3339.39 words/sec, time elapsed 248.57 sec\n",
            "epoch 2, iter 650, avg. loss 3.35, avg. ppl 1.13 cum. examples 1600, speed 3323.58 words/sec, time elapsed 251.21 sec\n",
            "epoch 2, iter 660, avg. loss 4.35, avg. ppl 1.17 cum. examples 1920, speed 3218.75 words/sec, time elapsed 254.03 sec\n",
            "epoch 2, iter 670, avg. loss 2.92, avg. ppl 1.11 cum. examples 2240, speed 3574.50 words/sec, time elapsed 256.50 sec\n",
            "epoch 2, iter 680, avg. loss 3.53, avg. ppl 1.13 cum. examples 2560, speed 3527.49 words/sec, time elapsed 259.16 sec\n",
            "epoch 2, iter 690, avg. loss 3.86, avg. ppl 1.14 cum. examples 2880, speed 3671.81 words/sec, time elapsed 261.74 sec\n",
            "epoch 2, iter 700, avg. loss 3.20, avg. ppl 1.11 cum. examples 3200, speed 3594.61 words/sec, time elapsed 264.43 sec\n",
            "epoch 2, iter 710, avg. loss 2.96, avg. ppl 1.11 cum. examples 3520, speed 3592.95 words/sec, time elapsed 266.99 sec\n",
            "epoch 2, iter 720, avg. loss 3.50, avg. ppl 1.13 cum. examples 3840, speed 3255.74 words/sec, time elapsed 269.82 sec\n",
            "epoch 2, iter 730, avg. loss 3.87, avg. ppl 1.14 cum. examples 4160, speed 3828.21 words/sec, time elapsed 272.21 sec\n",
            "epoch 2, iter 740, avg. loss 2.75, avg. ppl 1.10 cum. examples 4480, speed 3708.94 words/sec, time elapsed 274.73 sec\n",
            "epoch 2, iter 750, avg. loss 3.31, avg. ppl 1.12 cum. examples 4800, speed 3812.37 words/sec, time elapsed 277.10 sec\n",
            "epoch 2, iter 760, avg. loss 3.12, avg. ppl 1.11 cum. examples 5120, speed 3538.62 words/sec, time elapsed 279.69 sec\n",
            "epoch 2, iter 770, avg. loss 2.93, avg. ppl 1.10 cum. examples 5440, speed 3274.92 words/sec, time elapsed 282.57 sec\n",
            "epoch 2, iter 780, avg. loss 2.58, avg. ppl 1.09 cum. examples 5760, speed 3650.13 words/sec, time elapsed 285.15 sec\n",
            "epoch 2, iter 790, avg. loss 2.57, avg. ppl 1.09 cum. examples 6080, speed 3528.09 words/sec, time elapsed 287.72 sec\n",
            "epoch 2, iter 800, avg. loss 2.96, avg. ppl 1.11 cum. examples 6400, speed 3930.47 words/sec, time elapsed 290.04 sec\n",
            "epoch 2, iter 800, cum. loss 3.37, cum. ppl 1.12 cum. examples 6400\n",
            "begin validation ...\n",
            "validation: iter 800, dev. ppl 1.113222\n",
            "save currently the best model to [model.bin]\n",
            "save model parameters to [model.bin]\n",
            "epoch 2, iter 810, avg. loss 3.12, avg. ppl 1.11 cum. examples 320, speed 289.79 words/sec, time elapsed 322.79 sec\n",
            "epoch 2, iter 820, avg. loss 2.70, avg. ppl 1.10 cum. examples 640, speed 3843.19 words/sec, time elapsed 325.24 sec\n",
            "epoch 2, iter 830, avg. loss 2.97, avg. ppl 1.11 cum. examples 960, speed 3810.13 words/sec, time elapsed 327.60 sec\n",
            "epoch 2, iter 840, avg. loss 2.95, avg. ppl 1.11 cum. examples 1280, speed 3716.93 words/sec, time elapsed 330.06 sec\n",
            "epoch 2, iter 850, avg. loss 2.99, avg. ppl 1.11 cum. examples 1600, speed 3165.08 words/sec, time elapsed 332.98 sec\n",
            "epoch 2, iter 860, avg. loss 3.43, avg. ppl 1.13 cum. examples 1920, speed 3591.07 words/sec, time elapsed 335.52 sec\n",
            "epoch 2, iter 870, avg. loss 2.78, avg. ppl 1.10 cum. examples 2240, speed 3373.73 words/sec, time elapsed 338.25 sec\n",
            "epoch 2, iter 880, avg. loss 2.64, avg. ppl 1.10 cum. examples 2560, speed 3363.47 words/sec, time elapsed 340.92 sec\n",
            "epoch 2, iter 890, avg. loss 3.48, avg. ppl 1.13 cum. examples 2880, speed 3576.67 words/sec, time elapsed 343.47 sec\n",
            "epoch 2, iter 900, avg. loss 2.38, avg. ppl 1.09 cum. examples 3200, speed 3718.21 words/sec, time elapsed 345.85 sec\n",
            "epoch 2, iter 910, avg. loss 2.31, avg. ppl 1.09 cum. examples 3520, speed 3447.07 words/sec, time elapsed 348.48 sec\n",
            "epoch 2, iter 920, avg. loss 2.75, avg. ppl 1.10 cum. examples 3840, speed 3914.39 words/sec, time elapsed 350.90 sec\n",
            "epoch 2, iter 930, avg. loss 2.47, avg. ppl 1.09 cum. examples 4135, speed 3518.11 words/sec, time elapsed 353.29 sec\n",
            "epoch 3, iter 940, avg. loss 2.56, avg. ppl 1.09 cum. examples 4455, speed 3662.43 words/sec, time elapsed 355.77 sec\n",
            "epoch 3, iter 950, avg. loss 2.33, avg. ppl 1.08 cum. examples 4775, speed 3648.54 words/sec, time elapsed 358.36 sec\n",
            "epoch 3, iter 960, avg. loss 2.12, avg. ppl 1.08 cum. examples 5095, speed 3738.68 words/sec, time elapsed 360.85 sec\n",
            "epoch 3, iter 970, avg. loss 1.77, avg. ppl 1.07 cum. examples 5415, speed 3584.30 words/sec, time elapsed 363.35 sec\n",
            "epoch 3, iter 980, avg. loss 2.37, avg. ppl 1.08 cum. examples 5735, speed 3489.09 words/sec, time elapsed 366.15 sec\n",
            "epoch 3, iter 990, avg. loss 3.13, avg. ppl 1.11 cum. examples 6055, speed 3785.85 words/sec, time elapsed 368.62 sec\n",
            "epoch 3, iter 1000, avg. loss 2.35, avg. ppl 1.09 cum. examples 6375, speed 3790.43 words/sec, time elapsed 371.00 sec\n",
            "epoch 3, iter 1000, cum. loss 2.68, cum. ppl 1.10 cum. examples 6375\n",
            "begin validation ...\n",
            "validation: iter 1000, dev. ppl 1.072399\n",
            "save currently the best model to [model.bin]\n",
            "save model parameters to [model.bin]\n",
            "epoch 3, iter 1010, avg. loss 2.20, avg. ppl 1.08 cum. examples 320, speed 301.86 words/sec, time elapsed 402.73 sec\n",
            "epoch 3, iter 1020, avg. loss 2.60, avg. ppl 1.10 cum. examples 640, speed 3473.98 words/sec, time elapsed 405.35 sec\n",
            "epoch 3, iter 1030, avg. loss 2.71, avg. ppl 1.10 cum. examples 960, speed 3707.81 words/sec, time elapsed 407.77 sec\n",
            "epoch 3, iter 1040, avg. loss 2.11, avg. ppl 1.08 cum. examples 1280, speed 3586.50 words/sec, time elapsed 410.30 sec\n",
            "epoch 3, iter 1050, avg. loss 1.93, avg. ppl 1.07 cum. examples 1600, speed 3254.53 words/sec, time elapsed 413.13 sec\n",
            "epoch 3, iter 1060, avg. loss 2.02, avg. ppl 1.07 cum. examples 1920, speed 3647.62 words/sec, time elapsed 415.65 sec\n",
            "epoch 3, iter 1070, avg. loss 1.96, avg. ppl 1.07 cum. examples 2240, speed 3567.15 words/sec, time elapsed 418.35 sec\n",
            "epoch 3, iter 1080, avg. loss 1.87, avg. ppl 1.07 cum. examples 2560, speed 3594.76 words/sec, time elapsed 420.90 sec\n",
            "epoch 3, iter 1090, avg. loss 1.89, avg. ppl 1.07 cum. examples 2880, speed 3715.32 words/sec, time elapsed 423.33 sec\n",
            "epoch 3, iter 1100, avg. loss 1.94, avg. ppl 1.07 cum. examples 3200, speed 3402.04 words/sec, time elapsed 426.09 sec\n",
            "epoch 3, iter 1110, avg. loss 2.09, avg. ppl 1.07 cum. examples 3520, speed 3027.99 words/sec, time elapsed 429.19 sec\n",
            "epoch 3, iter 1120, avg. loss 2.33, avg. ppl 1.09 cum. examples 3840, speed 3712.31 words/sec, time elapsed 431.62 sec\n",
            "epoch 3, iter 1130, avg. loss 1.79, avg. ppl 1.06 cum. examples 4160, speed 3524.35 words/sec, time elapsed 434.21 sec\n",
            "epoch 3, iter 1140, avg. loss 2.23, avg. ppl 1.08 cum. examples 4480, speed 3511.17 words/sec, time elapsed 436.92 sec\n",
            "epoch 3, iter 1150, avg. loss 1.94, avg. ppl 1.07 cum. examples 4800, speed 3588.54 words/sec, time elapsed 439.49 sec\n",
            "epoch 3, iter 1160, avg. loss 1.98, avg. ppl 1.07 cum. examples 5120, speed 3532.34 words/sec, time elapsed 442.22 sec\n",
            "epoch 3, iter 1170, avg. loss 1.99, avg. ppl 1.07 cum. examples 5440, speed 3656.90 words/sec, time elapsed 444.64 sec\n",
            "epoch 3, iter 1180, avg. loss 1.88, avg. ppl 1.07 cum. examples 5760, speed 3298.83 words/sec, time elapsed 447.43 sec\n",
            "epoch 3, iter 1190, avg. loss 1.95, avg. ppl 1.07 cum. examples 6080, speed 3447.35 words/sec, time elapsed 450.09 sec\n",
            "epoch 3, iter 1200, avg. loss 1.98, avg. ppl 1.07 cum. examples 6400, speed 3609.61 words/sec, time elapsed 452.65 sec\n",
            "epoch 3, iter 1200, cum. loss 2.07, cum. ppl 1.07 cum. examples 6400\n",
            "begin validation ...\n",
            "validation: iter 1200, dev. ppl 1.053285\n",
            "save currently the best model to [model.bin]\n",
            "save model parameters to [model.bin]\n",
            "epoch 3, iter 1210, avg. loss 1.58, avg. ppl 1.06 cum. examples 320, speed 289.00 words/sec, time elapsed 483.44 sec\n",
            "epoch 3, iter 1220, avg. loss 1.35, avg. ppl 1.05 cum. examples 640, speed 3689.10 words/sec, time elapsed 485.98 sec\n",
            "epoch 3, iter 1230, avg. loss 1.60, avg. ppl 1.06 cum. examples 960, speed 3919.82 words/sec, time elapsed 488.38 sec\n",
            "epoch 3, iter 1240, avg. loss 1.76, avg. ppl 1.06 cum. examples 1280, speed 3795.69 words/sec, time elapsed 490.77 sec\n",
            "epoch 3, iter 1250, avg. loss 1.88, avg. ppl 1.06 cum. examples 1600, speed 3608.28 words/sec, time elapsed 493.52 sec\n",
            "epoch 3, iter 1260, avg. loss 1.89, avg. ppl 1.07 cum. examples 1920, speed 3351.80 words/sec, time elapsed 496.34 sec\n",
            "epoch 3, iter 1270, avg. loss 2.76, avg. ppl 1.10 cum. examples 2240, speed 3456.97 words/sec, time elapsed 499.11 sec\n",
            "epoch 3, iter 1280, avg. loss 1.61, avg. ppl 1.06 cum. examples 2560, speed 3749.34 words/sec, time elapsed 501.51 sec\n",
            "epoch 3, iter 1290, avg. loss 2.02, avg. ppl 1.08 cum. examples 2880, speed 3683.21 words/sec, time elapsed 503.91 sec\n",
            "epoch 3, iter 1300, avg. loss 1.75, avg. ppl 1.06 cum. examples 3200, speed 3370.37 words/sec, time elapsed 506.58 sec\n",
            "epoch 3, iter 1310, avg. loss 2.14, avg. ppl 1.08 cum. examples 3520, speed 3509.68 words/sec, time elapsed 509.25 sec\n",
            "epoch 3, iter 1320, avg. loss 1.74, avg. ppl 1.06 cum. examples 3840, speed 3609.26 words/sec, time elapsed 511.76 sec\n",
            "epoch 3, iter 1330, avg. loss 1.98, avg. ppl 1.07 cum. examples 4160, speed 3631.27 words/sec, time elapsed 514.22 sec\n",
            "epoch 3, iter 1340, avg. loss 1.91, avg. ppl 1.07 cum. examples 4480, speed 3503.43 words/sec, time elapsed 516.78 sec\n",
            "epoch 3, iter 1350, avg. loss 1.82, avg. ppl 1.07 cum. examples 4800, speed 3024.28 words/sec, time elapsed 519.83 sec\n",
            "epoch 3, iter 1360, avg. loss 1.59, avg. ppl 1.06 cum. examples 5120, speed 3424.00 words/sec, time elapsed 522.51 sec\n",
            "epoch 3, iter 1370, avg. loss 1.44, avg. ppl 1.05 cum. examples 5440, speed 3674.52 words/sec, time elapsed 524.92 sec\n",
            "epoch 3, iter 1380, avg. loss 2.25, avg. ppl 1.08 cum. examples 5760, speed 3419.77 words/sec, time elapsed 527.64 sec\n",
            "epoch 3, iter 1390, avg. loss 1.69, avg. ppl 1.06 cum. examples 6080, speed 3601.21 words/sec, time elapsed 530.11 sec\n",
            "epoch 4, iter 1400, avg. loss 2.02, avg. ppl 1.07 cum. examples 6375, speed 3290.42 words/sec, time elapsed 532.83 sec\n",
            "epoch 4, iter 1400, cum. loss 1.84, cum. ppl 1.07 cum. examples 6375\n",
            "begin validation ...\n",
            "validation: iter 1400, dev. ppl 1.086972\n",
            "hit patience 1\n",
            "hit #1 trial\n",
            "load previously best model and decay learning rate to 0.000250\n",
            "restore parameters of the optimizers\n",
            "epoch 4, iter 1410, avg. loss 1.38, avg. ppl 1.05 cum. examples 320, speed 328.90 words/sec, time elapsed 561.10 sec\n",
            "epoch 4, iter 1420, avg. loss 1.01, avg. ppl 1.04 cum. examples 640, speed 3881.45 words/sec, time elapsed 563.51 sec\n",
            "epoch 4, iter 1430, avg. loss 1.69, avg. ppl 1.06 cum. examples 960, speed 3053.07 words/sec, time elapsed 566.58 sec\n",
            "epoch 4, iter 1440, avg. loss 2.02, avg. ppl 1.07 cum. examples 1280, speed 3350.87 words/sec, time elapsed 569.43 sec\n",
            "epoch 4, iter 1450, avg. loss 1.32, avg. ppl 1.05 cum. examples 1600, speed 3476.94 words/sec, time elapsed 572.09 sec\n",
            "epoch 4, iter 1460, avg. loss 1.25, avg. ppl 1.05 cum. examples 1920, speed 3584.12 words/sec, time elapsed 574.50 sec\n",
            "epoch 4, iter 1470, avg. loss 0.99, avg. ppl 1.04 cum. examples 2240, speed 3749.98 words/sec, time elapsed 576.95 sec\n",
            "epoch 4, iter 1480, avg. loss 1.20, avg. ppl 1.04 cum. examples 2560, speed 3289.41 words/sec, time elapsed 579.78 sec\n",
            "epoch 4, iter 1490, avg. loss 1.30, avg. ppl 1.05 cum. examples 2880, speed 3496.11 words/sec, time elapsed 582.39 sec\n",
            "epoch 4, iter 1500, avg. loss 2.09, avg. ppl 1.08 cum. examples 3200, speed 3523.75 words/sec, time elapsed 584.98 sec\n",
            "epoch 4, iter 1510, avg. loss 1.06, avg. ppl 1.04 cum. examples 3520, speed 3569.32 words/sec, time elapsed 587.49 sec\n",
            "epoch 4, iter 1520, avg. loss 0.98, avg. ppl 1.03 cum. examples 3840, speed 3488.32 words/sec, time elapsed 590.14 sec\n",
            "epoch 4, iter 1530, avg. loss 1.32, avg. ppl 1.05 cum. examples 4160, speed 3414.32 words/sec, time elapsed 592.83 sec\n",
            "epoch 4, iter 1540, avg. loss 1.43, avg. ppl 1.05 cum. examples 4480, speed 3521.12 words/sec, time elapsed 595.39 sec\n",
            "epoch 4, iter 1550, avg. loss 1.23, avg. ppl 1.04 cum. examples 4800, speed 3019.74 words/sec, time elapsed 598.37 sec\n",
            "epoch 4, iter 1560, avg. loss 1.03, avg. ppl 1.04 cum. examples 5120, speed 3601.98 words/sec, time elapsed 600.86 sec\n",
            "epoch 4, iter 1570, avg. loss 1.13, avg. ppl 1.04 cum. examples 5440, speed 3721.73 words/sec, time elapsed 603.25 sec\n",
            "epoch 4, iter 1580, avg. loss 1.09, avg. ppl 1.04 cum. examples 5760, speed 3233.81 words/sec, time elapsed 605.95 sec\n",
            "epoch 4, iter 1590, avg. loss 1.37, avg. ppl 1.05 cum. examples 6080, speed 3809.25 words/sec, time elapsed 608.35 sec\n",
            "epoch 4, iter 1600, avg. loss 1.24, avg. ppl 1.04 cum. examples 6400, speed 3673.56 words/sec, time elapsed 610.91 sec\n",
            "epoch 4, iter 1600, cum. loss 1.30, cum. ppl 1.05 cum. examples 6400\n",
            "begin validation ...\n",
            "validation: iter 1600, dev. ppl 1.029924\n",
            "save currently the best model to [model.bin]\n",
            "save model parameters to [model.bin]\n",
            "epoch 4, iter 1610, avg. loss 0.89, avg. ppl 1.03 cum. examples 320, speed 290.38 words/sec, time elapsed 642.70 sec\n",
            "epoch 4, iter 1620, avg. loss 1.10, avg. ppl 1.04 cum. examples 640, speed 3748.78 words/sec, time elapsed 645.18 sec\n",
            "epoch 4, iter 1630, avg. loss 1.14, avg. ppl 1.04 cum. examples 960, speed 3695.42 words/sec, time elapsed 647.68 sec\n",
            "epoch 4, iter 1640, avg. loss 1.09, avg. ppl 1.04 cum. examples 1280, speed 3234.05 words/sec, time elapsed 650.47 sec\n",
            "epoch 4, iter 1650, avg. loss 1.45, avg. ppl 1.05 cum. examples 1600, speed 3342.41 words/sec, time elapsed 653.30 sec\n",
            "epoch 4, iter 1660, avg. loss 0.82, avg. ppl 1.03 cum. examples 1920, speed 3667.08 words/sec, time elapsed 655.86 sec\n",
            "epoch 4, iter 1670, avg. loss 1.38, avg. ppl 1.05 cum. examples 2240, speed 3924.01 words/sec, time elapsed 658.38 sec\n",
            "epoch 4, iter 1680, avg. loss 1.35, avg. ppl 1.05 cum. examples 2560, speed 3578.24 words/sec, time elapsed 661.05 sec\n",
            "epoch 4, iter 1690, avg. loss 0.97, avg. ppl 1.03 cum. examples 2880, speed 3518.39 words/sec, time elapsed 663.66 sec\n",
            "epoch 4, iter 1700, avg. loss 0.94, avg. ppl 1.03 cum. examples 3200, speed 3365.83 words/sec, time elapsed 666.36 sec\n",
            "epoch 4, iter 1710, avg. loss 1.24, avg. ppl 1.04 cum. examples 3520, speed 3694.56 words/sec, time elapsed 668.95 sec\n",
            "epoch 4, iter 1720, avg. loss 1.72, avg. ppl 1.06 cum. examples 3840, speed 3567.36 words/sec, time elapsed 671.54 sec\n",
            "epoch 4, iter 1730, avg. loss 1.36, avg. ppl 1.05 cum. examples 4160, speed 3573.51 words/sec, time elapsed 674.09 sec\n",
            "epoch 4, iter 1740, avg. loss 1.03, avg. ppl 1.04 cum. examples 4480, speed 3820.56 words/sec, time elapsed 676.49 sec\n",
            "epoch 4, iter 1750, avg. loss 1.00, avg. ppl 1.04 cum. examples 4800, speed 3347.81 words/sec, time elapsed 679.24 sec\n",
            "epoch 4, iter 1760, avg. loss 0.94, avg. ppl 1.03 cum. examples 5120, speed 3611.70 words/sec, time elapsed 681.81 sec\n",
            "epoch 4, iter 1770, avg. loss 1.13, avg. ppl 1.04 cum. examples 5440, speed 3706.75 words/sec, time elapsed 684.20 sec\n",
            "epoch 4, iter 1780, avg. loss 0.98, avg. ppl 1.03 cum. examples 5760, speed 3536.73 words/sec, time elapsed 686.81 sec\n",
            "epoch 4, iter 1790, avg. loss 1.12, avg. ppl 1.04 cum. examples 6080, speed 3486.43 words/sec, time elapsed 689.45 sec\n",
            "epoch 4, iter 1800, avg. loss 0.86, avg. ppl 1.03 cum. examples 6400, speed 3709.25 words/sec, time elapsed 691.92 sec\n",
            "epoch 4, iter 1800, cum. loss 1.13, cum. ppl 1.04 cum. examples 6400\n",
            "begin validation ...\n",
            "validation: iter 1800, dev. ppl 1.025019\n",
            "save currently the best model to [model.bin]\n",
            "save model parameters to [model.bin]\n",
            "epoch 4, iter 1810, avg. loss 1.12, avg. ppl 1.04 cum. examples 320, speed 294.79 words/sec, time elapsed 723.23 sec\n",
            "epoch 4, iter 1820, avg. loss 1.04, avg. ppl 1.04 cum. examples 640, speed 3451.39 words/sec, time elapsed 725.88 sec\n",
            "epoch 4, iter 1830, avg. loss 1.01, avg. ppl 1.04 cum. examples 960, speed 3773.77 words/sec, time elapsed 728.37 sec\n",
            "epoch 4, iter 1840, avg. loss 1.03, avg. ppl 1.04 cum. examples 1280, speed 3640.55 words/sec, time elapsed 730.89 sec\n",
            "epoch 4, iter 1850, avg. loss 1.22, avg. ppl 1.04 cum. examples 1600, speed 3264.16 words/sec, time elapsed 733.75 sec\n",
            "epoch 4, iter 1860, avg. loss 1.05, avg. ppl 1.04 cum. examples 1895, speed 3533.58 words/sec, time elapsed 736.20 sec\n",
            "epoch 5, iter 1870, avg. loss 0.80, avg. ppl 1.03 cum. examples 2215, speed 3478.44 words/sec, time elapsed 738.90 sec\n",
            "epoch 5, iter 1880, avg. loss 0.69, avg. ppl 1.02 cum. examples 2535, speed 3700.24 words/sec, time elapsed 741.44 sec\n",
            "epoch 5, iter 1890, avg. loss 0.78, avg. ppl 1.03 cum. examples 2855, speed 3718.49 words/sec, time elapsed 743.98 sec\n",
            "epoch 5, iter 1900, avg. loss 0.81, avg. ppl 1.03 cum. examples 3175, speed 3686.83 words/sec, time elapsed 746.54 sec\n",
            "epoch 5, iter 1910, avg. loss 0.62, avg. ppl 1.02 cum. examples 3495, speed 3365.34 words/sec, time elapsed 749.26 sec\n",
            "epoch 5, iter 1920, avg. loss 0.82, avg. ppl 1.03 cum. examples 3815, speed 3453.76 words/sec, time elapsed 751.95 sec\n",
            "epoch 5, iter 1930, avg. loss 0.64, avg. ppl 1.02 cum. examples 4135, speed 3587.69 words/sec, time elapsed 754.46 sec\n",
            "epoch 5, iter 1940, avg. loss 0.85, avg. ppl 1.03 cum. examples 4455, speed 3005.27 words/sec, time elapsed 757.56 sec\n",
            "epoch 5, iter 1950, avg. loss 1.06, avg. ppl 1.04 cum. examples 4775, speed 3471.07 words/sec, time elapsed 760.31 sec\n",
            "epoch 5, iter 1960, avg. loss 0.59, avg. ppl 1.02 cum. examples 5095, speed 3704.81 words/sec, time elapsed 762.76 sec\n",
            "epoch 5, iter 1970, avg. loss 1.08, avg. ppl 1.04 cum. examples 5415, speed 3841.60 words/sec, time elapsed 765.32 sec\n",
            "epoch 5, iter 1980, avg. loss 0.85, avg. ppl 1.03 cum. examples 5735, speed 3443.77 words/sec, time elapsed 767.98 sec\n",
            "epoch 5, iter 1990, avg. loss 1.10, avg. ppl 1.04 cum. examples 6055, speed 3701.69 words/sec, time elapsed 770.54 sec\n",
            "epoch 5, iter 2000, avg. loss 0.76, avg. ppl 1.03 cum. examples 6375, speed 3697.37 words/sec, time elapsed 773.07 sec\n",
            "epoch 5, iter 2000, cum. loss 0.90, cum. ppl 1.03 cum. examples 6375\n",
            "begin validation ...\n",
            "validation: iter 2000, dev. ppl 1.021948\n",
            "save currently the best model to [model.bin]\n",
            "save model parameters to [model.bin]\n",
            "epoch 5, iter 2010, avg. loss 0.62, avg. ppl 1.02 cum. examples 320, speed 284.66 words/sec, time elapsed 804.02 sec\n",
            "epoch 5, iter 2020, avg. loss 0.97, avg. ppl 1.03 cum. examples 640, speed 3251.68 words/sec, time elapsed 806.81 sec\n",
            "epoch 5, iter 2030, avg. loss 0.67, avg. ppl 1.02 cum. examples 960, speed 3299.15 words/sec, time elapsed 809.51 sec\n",
            "epoch 5, iter 2040, avg. loss 0.67, avg. ppl 1.02 cum. examples 1280, speed 3790.99 words/sec, time elapsed 811.91 sec\n",
            "epoch 5, iter 2050, avg. loss 0.63, avg. ppl 1.02 cum. examples 1600, speed 3471.06 words/sec, time elapsed 814.50 sec\n",
            "epoch 5, iter 2060, avg. loss 0.67, avg. ppl 1.02 cum. examples 1920, speed 3645.07 words/sec, time elapsed 817.00 sec\n",
            "epoch 5, iter 2070, avg. loss 0.62, avg. ppl 1.02 cum. examples 2240, speed 3344.59 words/sec, time elapsed 819.66 sec\n",
            "epoch 5, iter 2080, avg. loss 0.63, avg. ppl 1.02 cum. examples 2560, speed 3672.25 words/sec, time elapsed 822.16 sec\n",
            "epoch 5, iter 2090, avg. loss 0.73, avg. ppl 1.03 cum. examples 2880, speed 3403.81 words/sec, time elapsed 824.89 sec\n",
            "epoch 5, iter 2100, avg. loss 0.96, avg. ppl 1.03 cum. examples 3200, speed 3574.33 words/sec, time elapsed 827.48 sec\n",
            "epoch 5, iter 2110, avg. loss 0.65, avg. ppl 1.02 cum. examples 3520, speed 3521.45 words/sec, time elapsed 830.04 sec\n",
            "epoch 5, iter 2120, avg. loss 0.84, avg. ppl 1.03 cum. examples 3840, speed 3227.27 words/sec, time elapsed 832.83 sec\n",
            "epoch 5, iter 2130, avg. loss 0.55, avg. ppl 1.02 cum. examples 4160, speed 3423.79 words/sec, time elapsed 835.47 sec\n",
            "epoch 5, iter 2140, avg. loss 1.34, avg. ppl 1.05 cum. examples 4480, speed 3357.30 words/sec, time elapsed 838.12 sec\n",
            "epoch 5, iter 2150, avg. loss 0.78, avg. ppl 1.03 cum. examples 4800, speed 3678.57 words/sec, time elapsed 840.58 sec\n",
            "epoch 5, iter 2160, avg. loss 0.69, avg. ppl 1.02 cum. examples 5120, speed 3375.15 words/sec, time elapsed 843.33 sec\n",
            "epoch 5, iter 2170, avg. loss 1.08, avg. ppl 1.04 cum. examples 5440, speed 2674.57 words/sec, time elapsed 846.98 sec\n",
            "epoch 5, iter 2180, avg. loss 0.74, avg. ppl 1.03 cum. examples 5760, speed 3744.99 words/sec, time elapsed 849.49 sec\n",
            "epoch 5, iter 2190, avg. loss 0.60, avg. ppl 1.02 cum. examples 6080, speed 3446.23 words/sec, time elapsed 852.18 sec\n",
            "epoch 5, iter 2200, avg. loss 0.80, avg. ppl 1.03 cum. examples 6400, speed 3809.34 words/sec, time elapsed 854.70 sec\n",
            "epoch 5, iter 2200, cum. loss 0.76, cum. ppl 1.03 cum. examples 6400\n",
            "begin validation ...\n",
            "validation: iter 2200, dev. ppl 1.017100\n",
            "save currently the best model to [model.bin]\n",
            "save model parameters to [model.bin]\n",
            "epoch 5, iter 2210, avg. loss 0.67, avg. ppl 1.02 cum. examples 320, speed 267.82 words/sec, time elapsed 888.43 sec\n",
            "epoch 5, iter 2220, avg. loss 0.66, avg. ppl 1.02 cum. examples 640, speed 3690.60 words/sec, time elapsed 890.90 sec\n",
            "epoch 5, iter 2230, avg. loss 0.75, avg. ppl 1.03 cum. examples 960, speed 3413.97 words/sec, time elapsed 893.56 sec\n",
            "epoch 5, iter 2240, avg. loss 0.65, avg. ppl 1.02 cum. examples 1280, speed 3815.63 words/sec, time elapsed 896.03 sec\n",
            "epoch 5, iter 2250, avg. loss 0.80, avg. ppl 1.03 cum. examples 1600, speed 3671.64 words/sec, time elapsed 898.58 sec\n",
            "epoch 5, iter 2260, avg. loss 0.75, avg. ppl 1.03 cum. examples 1920, speed 3361.41 words/sec, time elapsed 901.27 sec\n",
            "epoch 5, iter 2270, avg. loss 0.57, avg. ppl 1.02 cum. examples 2240, speed 3643.29 words/sec, time elapsed 903.91 sec\n",
            "epoch 5, iter 2280, avg. loss 0.96, avg. ppl 1.03 cum. examples 2560, speed 3625.51 words/sec, time elapsed 906.55 sec\n",
            "epoch 5, iter 2290, avg. loss 0.45, avg. ppl 1.02 cum. examples 2880, speed 3582.86 words/sec, time elapsed 909.18 sec\n",
            "epoch 5, iter 2300, avg. loss 0.76, avg. ppl 1.03 cum. examples 3200, speed 3546.72 words/sec, time elapsed 911.72 sec\n",
            "epoch 5, iter 2310, avg. loss 0.64, avg. ppl 1.02 cum. examples 3520, speed 3561.19 words/sec, time elapsed 914.25 sec\n",
            "epoch 5, iter 2320, avg. loss 0.62, avg. ppl 1.02 cum. examples 3840, speed 3599.99 words/sec, time elapsed 916.70 sec\n",
            "epoch 6, iter 2330, avg. loss 0.77, avg. ppl 1.03 cum. examples 4135, speed 3433.99 words/sec, time elapsed 919.08 sec\n",
            "epoch 6, iter 2340, avg. loss 0.36, avg. ppl 1.01 cum. examples 4455, speed 3538.50 words/sec, time elapsed 921.66 sec\n",
            "epoch 6, iter 2350, avg. loss 1.05, avg. ppl 1.04 cum. examples 4775, speed 3557.22 words/sec, time elapsed 924.31 sec\n",
            "epoch 6, iter 2360, avg. loss 0.48, avg. ppl 1.02 cum. examples 5095, speed 3230.19 words/sec, time elapsed 927.14 sec\n",
            "epoch 6, iter 2370, avg. loss 0.57, avg. ppl 1.02 cum. examples 5415, speed 3781.98 words/sec, time elapsed 929.43 sec\n",
            "epoch 6, iter 2380, avg. loss 0.55, avg. ppl 1.02 cum. examples 5735, speed 3705.75 words/sec, time elapsed 931.79 sec\n",
            "epoch 6, iter 2390, avg. loss 0.52, avg. ppl 1.02 cum. examples 6055, speed 3594.32 words/sec, time elapsed 934.33 sec\n",
            "epoch 6, iter 2400, avg. loss 0.41, avg. ppl 1.01 cum. examples 6375, speed 3567.50 words/sec, time elapsed 937.00 sec\n",
            "epoch 6, iter 2400, cum. loss 0.65, cum. ppl 1.02 cum. examples 6375\n",
            "begin validation ...\n",
            "validation: iter 2400, dev. ppl 1.015601\n",
            "save currently the best model to [model.bin]\n",
            "save model parameters to [model.bin]\n",
            "epoch 6, iter 2410, avg. loss 0.52, avg. ppl 1.02 cum. examples 320, speed 304.22 words/sec, time elapsed 967.89 sec\n",
            "epoch 6, iter 2420, avg. loss 0.38, avg. ppl 1.01 cum. examples 640, speed 3467.44 words/sec, time elapsed 970.50 sec\n",
            "epoch 6, iter 2430, avg. loss 0.67, avg. ppl 1.02 cum. examples 960, speed 3631.48 words/sec, time elapsed 973.05 sec\n",
            "epoch 6, iter 2440, avg. loss 0.49, avg. ppl 1.02 cum. examples 1280, speed 3588.92 words/sec, time elapsed 975.73 sec\n",
            "epoch 6, iter 2450, avg. loss 0.89, avg. ppl 1.03 cum. examples 1600, speed 3530.87 words/sec, time elapsed 978.41 sec\n",
            "epoch 6, iter 2460, avg. loss 0.70, avg. ppl 1.02 cum. examples 1920, speed 3673.76 words/sec, time elapsed 981.03 sec\n",
            "epoch 6, iter 2470, avg. loss 0.63, avg. ppl 1.02 cum. examples 2240, speed 3887.62 words/sec, time elapsed 983.47 sec\n",
            "epoch 6, iter 2480, avg. loss 0.46, avg. ppl 1.02 cum. examples 2560, speed 3825.23 words/sec, time elapsed 985.80 sec\n",
            "epoch 6, iter 2490, avg. loss 0.53, avg. ppl 1.02 cum. examples 2880, speed 3447.57 words/sec, time elapsed 988.52 sec\n",
            "epoch 6, iter 2500, avg. loss 0.57, avg. ppl 1.02 cum. examples 3200, speed 3666.22 words/sec, time elapsed 991.24 sec\n",
            "epoch 6, iter 2510, avg. loss 0.59, avg. ppl 1.02 cum. examples 3520, speed 3485.82 words/sec, time elapsed 993.90 sec\n",
            "epoch 6, iter 2520, avg. loss 0.52, avg. ppl 1.02 cum. examples 3840, speed 3788.44 words/sec, time elapsed 996.35 sec\n",
            "epoch 6, iter 2530, avg. loss 0.51, avg. ppl 1.02 cum. examples 4160, speed 3484.78 words/sec, time elapsed 999.04 sec\n",
            "epoch 6, iter 2540, avg. loss 0.37, avg. ppl 1.01 cum. examples 4480, speed 3737.24 words/sec, time elapsed 1001.60 sec\n",
            "epoch 6, iter 2550, avg. loss 0.61, avg. ppl 1.02 cum. examples 4800, speed 3334.91 words/sec, time elapsed 1004.32 sec\n",
            "epoch 6, iter 2560, avg. loss 0.50, avg. ppl 1.02 cum. examples 5120, speed 3047.01 words/sec, time elapsed 1007.26 sec\n",
            "epoch 6, iter 2570, avg. loss 0.54, avg. ppl 1.02 cum. examples 5440, speed 3797.85 words/sec, time elapsed 1009.68 sec\n",
            "epoch 6, iter 2580, avg. loss 0.73, avg. ppl 1.03 cum. examples 5760, speed 3195.00 words/sec, time elapsed 1012.58 sec\n",
            "epoch 6, iter 2590, avg. loss 0.52, avg. ppl 1.02 cum. examples 6080, speed 3228.47 words/sec, time elapsed 1015.44 sec\n",
            "epoch 6, iter 2600, avg. loss 0.45, avg. ppl 1.02 cum. examples 6400, speed 3422.69 words/sec, time elapsed 1018.21 sec\n",
            "epoch 6, iter 2600, cum. loss 0.56, cum. ppl 1.02 cum. examples 6400\n",
            "begin validation ...\n",
            "validation: iter 2600, dev. ppl 1.014367\n",
            "save currently the best model to [model.bin]\n",
            "save model parameters to [model.bin]\n",
            "epoch 6, iter 2610, avg. loss 0.67, avg. ppl 1.02 cum. examples 320, speed 297.01 words/sec, time elapsed 1049.46 sec\n",
            "epoch 6, iter 2620, avg. loss 0.43, avg. ppl 1.02 cum. examples 640, speed 3671.51 words/sec, time elapsed 1051.87 sec\n",
            "epoch 6, iter 2630, avg. loss 0.56, avg. ppl 1.02 cum. examples 960, speed 3540.19 words/sec, time elapsed 1054.37 sec\n",
            "epoch 6, iter 2640, avg. loss 0.61, avg. ppl 1.02 cum. examples 1280, speed 3356.79 words/sec, time elapsed 1057.02 sec\n",
            "epoch 6, iter 2650, avg. loss 0.43, avg. ppl 1.01 cum. examples 1600, speed 3578.86 words/sec, time elapsed 1059.65 sec\n",
            "epoch 6, iter 2660, avg. loss 0.49, avg. ppl 1.02 cum. examples 1920, speed 3637.19 words/sec, time elapsed 1062.17 sec\n",
            "epoch 6, iter 2670, avg. loss 0.55, avg. ppl 1.02 cum. examples 2240, speed 3659.22 words/sec, time elapsed 1064.69 sec\n",
            "epoch 6, iter 2680, avg. loss 0.40, avg. ppl 1.01 cum. examples 2560, speed 3496.12 words/sec, time elapsed 1067.31 sec\n",
            "epoch 6, iter 2690, avg. loss 0.65, avg. ppl 1.02 cum. examples 2880, speed 3421.98 words/sec, time elapsed 1070.03 sec\n",
            "epoch 6, iter 2700, avg. loss 0.56, avg. ppl 1.02 cum. examples 3200, speed 3706.07 words/sec, time elapsed 1072.42 sec\n",
            "epoch 6, iter 2710, avg. loss 0.57, avg. ppl 1.02 cum. examples 3520, speed 3286.34 words/sec, time elapsed 1075.19 sec\n",
            "epoch 6, iter 2720, avg. loss 0.58, avg. ppl 1.02 cum. examples 3840, speed 3437.29 words/sec, time elapsed 1077.68 sec\n",
            "epoch 6, iter 2730, avg. loss 0.57, avg. ppl 1.02 cum. examples 4160, speed 3538.61 words/sec, time elapsed 1080.30 sec\n",
            "epoch 6, iter 2740, avg. loss 0.36, avg. ppl 1.01 cum. examples 4480, speed 3708.12 words/sec, time elapsed 1082.66 sec\n",
            "epoch 6, iter 2750, avg. loss 0.75, avg. ppl 1.03 cum. examples 4800, speed 3459.77 words/sec, time elapsed 1085.32 sec\n",
            "epoch 6, iter 2760, avg. loss 0.59, avg. ppl 1.02 cum. examples 5120, speed 3502.93 words/sec, time elapsed 1087.90 sec\n",
            "epoch 6, iter 2770, avg. loss 0.76, avg. ppl 1.03 cum. examples 5440, speed 3344.46 words/sec, time elapsed 1090.72 sec\n",
            "epoch 6, iter 2780, avg. loss 0.74, avg. ppl 1.02 cum. examples 5760, speed 3184.59 words/sec, time elapsed 1093.80 sec\n",
            "epoch 6, iter 2790, avg. loss 0.40, avg. ppl 1.01 cum. examples 6055, speed 3381.53 words/sec, time elapsed 1096.36 sec\n",
            "epoch 7, iter 2800, avg. loss 0.37, avg. ppl 1.01 cum. examples 6375, speed 3553.41 words/sec, time elapsed 1098.96 sec\n",
            "epoch 7, iter 2800, cum. loss 0.55, cum. ppl 1.02 cum. examples 6375\n",
            "begin validation ...\n",
            "validation: iter 2800, dev. ppl 1.012849\n",
            "save currently the best model to [model.bin]\n",
            "save model parameters to [model.bin]\n",
            "epoch 7, iter 2810, avg. loss 0.42, avg. ppl 1.02 cum. examples 320, speed 278.40 words/sec, time elapsed 1130.62 sec\n",
            "epoch 7, iter 2820, avg. loss 0.56, avg. ppl 1.02 cum. examples 640, speed 3505.79 words/sec, time elapsed 1133.34 sec\n",
            "epoch 7, iter 2830, avg. loss 0.29, avg. ppl 1.01 cum. examples 960, speed 3828.93 words/sec, time elapsed 1135.71 sec\n",
            "epoch 7, iter 2840, avg. loss 0.54, avg. ppl 1.02 cum. examples 1280, speed 3356.21 words/sec, time elapsed 1138.45 sec\n",
            "epoch 7, iter 2850, avg. loss 0.41, avg. ppl 1.01 cum. examples 1600, speed 3552.78 words/sec, time elapsed 1141.07 sec\n",
            "epoch 7, iter 2860, avg. loss 0.53, avg. ppl 1.02 cum. examples 1920, speed 3679.04 words/sec, time elapsed 1143.48 sec\n",
            "epoch 7, iter 2870, avg. loss 0.41, avg. ppl 1.01 cum. examples 2240, speed 3699.02 words/sec, time elapsed 1145.97 sec\n",
            "epoch 7, iter 2880, avg. loss 0.43, avg. ppl 1.01 cum. examples 2560, speed 3363.40 words/sec, time elapsed 1148.73 sec\n",
            "epoch 7, iter 2890, avg. loss 0.58, avg. ppl 1.02 cum. examples 2880, speed 3606.59 words/sec, time elapsed 1151.29 sec\n",
            "epoch 7, iter 2900, avg. loss 0.40, avg. ppl 1.01 cum. examples 3200, speed 3192.00 words/sec, time elapsed 1154.02 sec\n",
            "epoch 7, iter 2910, avg. loss 0.27, avg. ppl 1.01 cum. examples 3520, speed 3550.30 words/sec, time elapsed 1156.61 sec\n",
            "epoch 7, iter 2920, avg. loss 0.43, avg. ppl 1.02 cum. examples 3840, speed 3348.07 words/sec, time elapsed 1159.32 sec\n",
            "epoch 7, iter 2930, avg. loss 0.67, avg. ppl 1.02 cum. examples 4160, speed 3605.95 words/sec, time elapsed 1161.97 sec\n",
            "epoch 7, iter 2940, avg. loss 0.40, avg. ppl 1.01 cum. examples 4480, speed 3641.29 words/sec, time elapsed 1164.51 sec\n",
            "epoch 7, iter 2950, avg. loss 0.47, avg. ppl 1.02 cum. examples 4800, speed 3712.86 words/sec, time elapsed 1167.01 sec\n",
            "epoch 7, iter 2960, avg. loss 0.32, avg. ppl 1.01 cum. examples 5120, speed 3505.70 words/sec, time elapsed 1169.68 sec\n",
            "epoch 7, iter 2970, avg. loss 0.39, avg. ppl 1.01 cum. examples 5440, speed 3842.36 words/sec, time elapsed 1172.12 sec\n",
            "epoch 7, iter 2980, avg. loss 0.54, avg. ppl 1.02 cum. examples 5760, speed 3196.27 words/sec, time elapsed 1175.03 sec\n",
            "epoch 7, iter 2990, avg. loss 0.55, avg. ppl 1.02 cum. examples 6080, speed 3582.14 words/sec, time elapsed 1177.67 sec\n",
            "epoch 7, iter 3000, avg. loss 0.33, avg. ppl 1.01 cum. examples 6400, speed 3494.76 words/sec, time elapsed 1180.21 sec\n",
            "epoch 7, iter 3000, cum. loss 0.45, cum. ppl 1.02 cum. examples 6400\n",
            "begin validation ...\n",
            "validation: iter 3000, dev. ppl 1.011766\n",
            "save currently the best model to [model.bin]\n",
            "save model parameters to [model.bin]\n",
            "epoch 7, iter 3010, avg. loss 0.51, avg. ppl 1.02 cum. examples 320, speed 284.72 words/sec, time elapsed 1210.93 sec\n",
            "epoch 7, iter 3020, avg. loss 0.42, avg. ppl 1.01 cum. examples 640, speed 3723.48 words/sec, time elapsed 1213.46 sec\n",
            "epoch 7, iter 3030, avg. loss 0.51, avg. ppl 1.02 cum. examples 960, speed 3403.01 words/sec, time elapsed 1216.13 sec\n",
            "epoch 7, iter 3040, avg. loss 0.38, avg. ppl 1.01 cum. examples 1280, speed 3282.04 words/sec, time elapsed 1218.89 sec\n",
            "epoch 7, iter 3050, avg. loss 1.02, avg. ppl 1.04 cum. examples 1600, speed 3008.67 words/sec, time elapsed 1221.84 sec\n",
            "epoch 7, iter 3060, avg. loss 0.39, avg. ppl 1.01 cum. examples 1920, speed 3777.77 words/sec, time elapsed 1224.27 sec\n",
            "epoch 7, iter 3070, avg. loss 0.54, avg. ppl 1.02 cum. examples 2240, speed 3802.30 words/sec, time elapsed 1226.80 sec\n",
            "epoch 7, iter 3080, avg. loss 0.44, avg. ppl 1.02 cum. examples 2560, speed 3221.34 words/sec, time elapsed 1229.70 sec\n",
            "epoch 7, iter 3090, avg. loss 0.75, avg. ppl 1.03 cum. examples 2880, speed 3618.15 words/sec, time elapsed 1232.35 sec\n",
            "epoch 7, iter 3100, avg. loss 0.57, avg. ppl 1.02 cum. examples 3200, speed 3672.60 words/sec, time elapsed 1234.93 sec\n",
            "epoch 7, iter 3110, avg. loss 0.43, avg. ppl 1.01 cum. examples 3520, speed 3784.04 words/sec, time elapsed 1237.44 sec\n",
            "epoch 7, iter 3120, avg. loss 0.55, avg. ppl 1.02 cum. examples 3840, speed 3457.10 words/sec, time elapsed 1240.13 sec\n",
            "epoch 7, iter 3130, avg. loss 0.60, avg. ppl 1.02 cum. examples 4160, speed 3380.19 words/sec, time elapsed 1242.82 sec\n",
            "epoch 7, iter 3140, avg. loss 0.65, avg. ppl 1.02 cum. examples 4480, speed 3349.83 words/sec, time elapsed 1245.56 sec\n",
            "epoch 7, iter 3150, avg. loss 0.40, avg. ppl 1.01 cum. examples 4800, speed 3760.82 words/sec, time elapsed 1248.00 sec\n",
            "epoch 7, iter 3160, avg. loss 0.65, avg. ppl 1.02 cum. examples 5120, speed 3382.35 words/sec, time elapsed 1250.92 sec\n",
            "epoch 7, iter 3170, avg. loss 0.51, avg. ppl 1.02 cum. examples 5440, speed 3552.84 words/sec, time elapsed 1253.42 sec\n",
            "epoch 7, iter 3180, avg. loss 0.39, avg. ppl 1.01 cum. examples 5760, speed 3719.20 words/sec, time elapsed 1255.77 sec\n",
            "epoch 7, iter 3190, avg. loss 0.53, avg. ppl 1.02 cum. examples 6080, speed 3649.44 words/sec, time elapsed 1258.36 sec\n",
            "epoch 7, iter 3200, avg. loss 0.40, avg. ppl 1.01 cum. examples 6400, speed 3714.41 words/sec, time elapsed 1260.86 sec\n",
            "epoch 7, iter 3200, cum. loss 0.53, cum. ppl 1.02 cum. examples 6400\n",
            "begin validation ...\n",
            "validation: iter 3200, dev. ppl 1.012644\n",
            "hit patience 1\n",
            "hit #2 trial\n",
            "load previously best model and decay learning rate to 0.000125\n",
            "restore parameters of the optimizers\n",
            "epoch 7, iter 3210, avg. loss 0.41, avg. ppl 1.01 cum. examples 320, speed 333.43 words/sec, time elapsed 1289.19 sec\n",
            "epoch 7, iter 3220, avg. loss 0.29, avg. ppl 1.01 cum. examples 640, speed 3210.43 words/sec, time elapsed 1291.94 sec\n",
            "epoch 7, iter 3230, avg. loss 0.46, avg. ppl 1.02 cum. examples 960, speed 3578.58 words/sec, time elapsed 1294.44 sec\n",
            "epoch 7, iter 3240, avg. loss 0.34, avg. ppl 1.01 cum. examples 1280, speed 3537.04 words/sec, time elapsed 1297.03 sec\n",
            "epoch 7, iter 3250, avg. loss 0.45, avg. ppl 1.01 cum. examples 1600, speed 3530.90 words/sec, time elapsed 1299.76 sec\n",
            "epoch 8, iter 3260, avg. loss 0.13, avg. ppl 1.00 cum. examples 1895, speed 3683.16 words/sec, time elapsed 1302.02 sec\n",
            "epoch 8, iter 3270, avg. loss 0.29, avg. ppl 1.01 cum. examples 2215, speed 3118.21 words/sec, time elapsed 1304.97 sec\n",
            "epoch 8, iter 3280, avg. loss 0.18, avg. ppl 1.01 cum. examples 2535, speed 3540.93 words/sec, time elapsed 1307.61 sec\n",
            "epoch 8, iter 3290, avg. loss 0.35, avg. ppl 1.01 cum. examples 2855, speed 2950.55 words/sec, time elapsed 1310.65 sec\n",
            "epoch 8, iter 3300, avg. loss 0.20, avg. ppl 1.01 cum. examples 3175, speed 3484.35 words/sec, time elapsed 1313.23 sec\n",
            "epoch 8, iter 3310, avg. loss 0.31, avg. ppl 1.01 cum. examples 3495, speed 3571.53 words/sec, time elapsed 1315.91 sec\n",
            "epoch 8, iter 3320, avg. loss 0.37, avg. ppl 1.01 cum. examples 3815, speed 3539.58 words/sec, time elapsed 1318.56 sec\n",
            "epoch 8, iter 3330, avg. loss 0.22, avg. ppl 1.01 cum. examples 4135, speed 3713.53 words/sec, time elapsed 1321.04 sec\n",
            "epoch 8, iter 3340, avg. loss 0.13, avg. ppl 1.00 cum. examples 4455, speed 3747.89 words/sec, time elapsed 1323.56 sec\n",
            "epoch 8, iter 3350, avg. loss 0.30, avg. ppl 1.01 cum. examples 4775, speed 3523.76 words/sec, time elapsed 1326.26 sec\n",
            "epoch 8, iter 3360, avg. loss 0.21, avg. ppl 1.01 cum. examples 5095, speed 3392.25 words/sec, time elapsed 1328.94 sec\n",
            "epoch 8, iter 3370, avg. loss 0.15, avg. ppl 1.01 cum. examples 5415, speed 3681.76 words/sec, time elapsed 1331.51 sec\n",
            "epoch 8, iter 3380, avg. loss 0.25, avg. ppl 1.01 cum. examples 5735, speed 3580.21 words/sec, time elapsed 1334.13 sec\n",
            "epoch 8, iter 3390, avg. loss 0.21, avg. ppl 1.01 cum. examples 6055, speed 3747.23 words/sec, time elapsed 1336.51 sec\n",
            "epoch 8, iter 3400, avg. loss 0.24, avg. ppl 1.01 cum. examples 6375, speed 3280.58 words/sec, time elapsed 1339.28 sec\n",
            "epoch 8, iter 3400, cum. loss 0.28, cum. ppl 1.01 cum. examples 6375\n",
            "begin validation ...\n",
            "validation: iter 3400, dev. ppl 1.008486\n",
            "save currently the best model to [model.bin]\n",
            "save model parameters to [model.bin]\n",
            "epoch 8, iter 3410, avg. loss 0.22, avg. ppl 1.01 cum. examples 320, speed 264.76 words/sec, time elapsed 1372.74 sec\n",
            "epoch 8, iter 3420, avg. loss 0.22, avg. ppl 1.01 cum. examples 640, speed 3577.68 words/sec, time elapsed 1375.21 sec\n",
            "epoch 8, iter 3430, avg. loss 0.21, avg. ppl 1.01 cum. examples 960, speed 3546.23 words/sec, time elapsed 1377.71 sec\n",
            "epoch 8, iter 3440, avg. loss 0.37, avg. ppl 1.01 cum. examples 1280, speed 3531.53 words/sec, time elapsed 1380.30 sec\n",
            "epoch 8, iter 3450, avg. loss 0.37, avg. ppl 1.01 cum. examples 1600, speed 3590.81 words/sec, time elapsed 1382.80 sec\n",
            "epoch 8, iter 3460, avg. loss 0.62, avg. ppl 1.02 cum. examples 1920, speed 3904.82 words/sec, time elapsed 1385.22 sec\n",
            "epoch 8, iter 3470, avg. loss 0.25, avg. ppl 1.01 cum. examples 2240, speed 3649.90 words/sec, time elapsed 1387.66 sec\n",
            "epoch 8, iter 3480, avg. loss 0.32, avg. ppl 1.01 cum. examples 2560, speed 3715.45 words/sec, time elapsed 1390.18 sec\n",
            "epoch 8, iter 3490, avg. loss 0.17, avg. ppl 1.01 cum. examples 2880, speed 3677.13 words/sec, time elapsed 1392.74 sec\n",
            "epoch 8, iter 3500, avg. loss 0.70, avg. ppl 1.03 cum. examples 3200, speed 3704.23 words/sec, time elapsed 1395.18 sec\n",
            "epoch 8, iter 3510, avg. loss 0.30, avg. ppl 1.01 cum. examples 3520, speed 3259.46 words/sec, time elapsed 1398.04 sec\n",
            "epoch 8, iter 3520, avg. loss 0.37, avg. ppl 1.01 cum. examples 3840, speed 3759.24 words/sec, time elapsed 1400.50 sec\n",
            "epoch 8, iter 3530, avg. loss 0.14, avg. ppl 1.00 cum. examples 4160, speed 3644.68 words/sec, time elapsed 1402.93 sec\n",
            "epoch 8, iter 3540, avg. loss 0.18, avg. ppl 1.01 cum. examples 4480, speed 3478.06 words/sec, time elapsed 1405.60 sec\n",
            "epoch 8, iter 3550, avg. loss 0.14, avg. ppl 1.00 cum. examples 4800, speed 4025.50 words/sec, time elapsed 1407.89 sec\n",
            "epoch 8, iter 3560, avg. loss 0.36, avg. ppl 1.01 cum. examples 5120, speed 3413.14 words/sec, time elapsed 1410.52 sec\n",
            "epoch 8, iter 3570, avg. loss 0.56, avg. ppl 1.02 cum. examples 5440, speed 3660.89 words/sec, time elapsed 1412.94 sec\n",
            "epoch 8, iter 3580, avg. loss 0.22, avg. ppl 1.01 cum. examples 5760, speed 3587.76 words/sec, time elapsed 1415.52 sec\n",
            "epoch 8, iter 3590, avg. loss 0.31, avg. ppl 1.01 cum. examples 6080, speed 3329.66 words/sec, time elapsed 1418.23 sec\n",
            "epoch 8, iter 3600, avg. loss 0.23, avg. ppl 1.01 cum. examples 6400, speed 3393.22 words/sec, time elapsed 1421.01 sec\n",
            "epoch 8, iter 3600, cum. loss 0.31, cum. ppl 1.01 cum. examples 6400\n",
            "begin validation ...\n",
            "validation: iter 3600, dev. ppl 1.005700\n",
            "save currently the best model to [model.bin]\n",
            "save model parameters to [model.bin]\n",
            "epoch 8, iter 3610, avg. loss 0.21, avg. ppl 1.01 cum. examples 320, speed 296.57 words/sec, time elapsed 1452.05 sec\n",
            "epoch 8, iter 3620, avg. loss 0.48, avg. ppl 1.02 cum. examples 640, speed 3345.31 words/sec, time elapsed 1454.98 sec\n",
            "epoch 8, iter 3630, avg. loss 0.26, avg. ppl 1.01 cum. examples 960, speed 3631.86 words/sec, time elapsed 1457.67 sec\n",
            "epoch 8, iter 3640, avg. loss 0.32, avg. ppl 1.01 cum. examples 1280, speed 3559.97 words/sec, time elapsed 1460.30 sec\n",
            "epoch 8, iter 3650, avg. loss 0.24, avg. ppl 1.01 cum. examples 1600, speed 3400.06 words/sec, time elapsed 1463.17 sec\n",
            "epoch 8, iter 3660, avg. loss 0.22, avg. ppl 1.01 cum. examples 1920, speed 3700.52 words/sec, time elapsed 1465.64 sec\n",
            "epoch 8, iter 3670, avg. loss 0.29, avg. ppl 1.01 cum. examples 2240, speed 3635.36 words/sec, time elapsed 1468.20 sec\n",
            "epoch 8, iter 3680, avg. loss 0.30, avg. ppl 1.01 cum. examples 2560, speed 3459.30 words/sec, time elapsed 1470.81 sec\n",
            "epoch 8, iter 3690, avg. loss 0.41, avg. ppl 1.01 cum. examples 2880, speed 3165.59 words/sec, time elapsed 1473.70 sec\n",
            "epoch 8, iter 3700, avg. loss 0.24, avg. ppl 1.01 cum. examples 3200, speed 3704.91 words/sec, time elapsed 1476.21 sec\n",
            "epoch 8, iter 3710, avg. loss 0.19, avg. ppl 1.01 cum. examples 3520, speed 3372.62 words/sec, time elapsed 1478.97 sec\n",
            "epoch 8, iter 3720, avg. loss 0.31, avg. ppl 1.01 cum. examples 3815, speed 3512.21 words/sec, time elapsed 1481.39 sec\n",
            "epoch 9, iter 3730, avg. loss 0.20, avg. ppl 1.01 cum. examples 4135, speed 3600.42 words/sec, time elapsed 1484.02 sec\n",
            "epoch 9, iter 3740, avg. loss 0.07, avg. ppl 1.00 cum. examples 4455, speed 3640.86 words/sec, time elapsed 1486.57 sec\n",
            "epoch 9, iter 3750, avg. loss 0.15, avg. ppl 1.01 cum. examples 4775, speed 3683.47 words/sec, time elapsed 1489.03 sec\n",
            "epoch 9, iter 3760, avg. loss 0.09, avg. ppl 1.00 cum. examples 5095, speed 3576.52 words/sec, time elapsed 1491.52 sec\n",
            "epoch 9, iter 3770, avg. loss 0.13, avg. ppl 1.00 cum. examples 5415, speed 3604.87 words/sec, time elapsed 1494.08 sec\n",
            "epoch 9, iter 3780, avg. loss 0.13, avg. ppl 1.00 cum. examples 5735, speed 3796.08 words/sec, time elapsed 1496.53 sec\n",
            "epoch 9, iter 3790, avg. loss 0.17, avg. ppl 1.01 cum. examples 6055, speed 3516.73 words/sec, time elapsed 1499.12 sec\n",
            "epoch 9, iter 3800, avg. loss 0.15, avg. ppl 1.01 cum. examples 6375, speed 3723.66 words/sec, time elapsed 1501.58 sec\n",
            "epoch 9, iter 3800, cum. loss 0.23, cum. ppl 1.01 cum. examples 6375\n",
            "begin validation ...\n",
            "validation: iter 3800, dev. ppl 1.004199\n",
            "save currently the best model to [model.bin]\n",
            "save model parameters to [model.bin]\n",
            "epoch 9, iter 3810, avg. loss 0.13, avg. ppl 1.00 cum. examples 320, speed 284.85 words/sec, time elapsed 1532.37 sec\n",
            "epoch 9, iter 3820, avg. loss 0.62, avg. ppl 1.02 cum. examples 640, speed 3581.15 words/sec, time elapsed 1534.86 sec\n",
            "epoch 9, iter 3830, avg. loss 0.16, avg. ppl 1.01 cum. examples 960, speed 3326.26 words/sec, time elapsed 1537.67 sec\n",
            "epoch 9, iter 3840, avg. loss 0.17, avg. ppl 1.01 cum. examples 1280, speed 3520.95 words/sec, time elapsed 1540.31 sec\n",
            "epoch 9, iter 3850, avg. loss 0.16, avg. ppl 1.01 cum. examples 1600, speed 3714.61 words/sec, time elapsed 1542.83 sec\n",
            "epoch 9, iter 3860, avg. loss 0.23, avg. ppl 1.01 cum. examples 1920, speed 3306.02 words/sec, time elapsed 1545.64 sec\n",
            "epoch 9, iter 3870, avg. loss 0.22, avg. ppl 1.01 cum. examples 2240, speed 3622.72 words/sec, time elapsed 1548.23 sec\n",
            "epoch 9, iter 3880, avg. loss 0.21, avg. ppl 1.01 cum. examples 2560, speed 3720.09 words/sec, time elapsed 1550.90 sec\n",
            "epoch 9, iter 3890, avg. loss 0.19, avg. ppl 1.01 cum. examples 2880, speed 3517.29 words/sec, time elapsed 1553.43 sec\n",
            "epoch 9, iter 3900, avg. loss 0.18, avg. ppl 1.01 cum. examples 3200, speed 3569.21 words/sec, time elapsed 1556.05 sec\n",
            "epoch 9, iter 3910, avg. loss 0.13, avg. ppl 1.00 cum. examples 3520, speed 3737.21 words/sec, time elapsed 1558.53 sec\n",
            "epoch 9, iter 3920, avg. loss 0.10, avg. ppl 1.00 cum. examples 3840, speed 3420.48 words/sec, time elapsed 1561.30 sec\n",
            "epoch 9, iter 3930, avg. loss 0.23, avg. ppl 1.01 cum. examples 4160, speed 3311.92 words/sec, time elapsed 1564.06 sec\n",
            "epoch 9, iter 3940, avg. loss 0.22, avg. ppl 1.01 cum. examples 4480, speed 3431.01 words/sec, time elapsed 1566.61 sec\n",
            "epoch 9, iter 3950, avg. loss 0.17, avg. ppl 1.01 cum. examples 4800, speed 3800.39 words/sec, time elapsed 1569.02 sec\n",
            "epoch 9, iter 3960, avg. loss 0.38, avg. ppl 1.01 cum. examples 5120, speed 3353.78 words/sec, time elapsed 1571.90 sec\n",
            "epoch 9, iter 3970, avg. loss 0.13, avg. ppl 1.00 cum. examples 5440, speed 3608.12 words/sec, time elapsed 1574.35 sec\n",
            "epoch 9, iter 3980, avg. loss 0.18, avg. ppl 1.01 cum. examples 5760, speed 3494.50 words/sec, time elapsed 1576.86 sec\n",
            "epoch 9, iter 3990, avg. loss 0.12, avg. ppl 1.00 cum. examples 6080, speed 3783.83 words/sec, time elapsed 1579.31 sec\n",
            "epoch 9, iter 4000, avg. loss 0.19, avg. ppl 1.01 cum. examples 6400, speed 3570.37 words/sec, time elapsed 1581.85 sec\n",
            "epoch 9, iter 4000, cum. loss 0.20, cum. ppl 1.01 cum. examples 6400\n",
            "begin validation ...\n",
            "validation: iter 4000, dev. ppl 1.005367\n",
            "hit patience 1\n",
            "hit #3 trial\n",
            "load previously best model and decay learning rate to 0.000063\n",
            "restore parameters of the optimizers\n",
            "epoch 9, iter 4010, avg. loss 0.14, avg. ppl 1.00 cum. examples 320, speed 334.39 words/sec, time elapsed 1610.24 sec\n",
            "epoch 9, iter 4020, avg. loss 0.17, avg. ppl 1.01 cum. examples 640, speed 3376.07 words/sec, time elapsed 1612.98 sec\n",
            "epoch 9, iter 4030, avg. loss 0.17, avg. ppl 1.01 cum. examples 960, speed 3746.13 words/sec, time elapsed 1615.45 sec\n",
            "epoch 9, iter 4040, avg. loss 0.11, avg. ppl 1.00 cum. examples 1280, speed 3660.35 words/sec, time elapsed 1617.93 sec\n",
            "epoch 9, iter 4050, avg. loss 0.15, avg. ppl 1.01 cum. examples 1600, speed 3402.33 words/sec, time elapsed 1620.60 sec\n",
            "epoch 9, iter 4060, avg. loss 0.08, avg. ppl 1.00 cum. examples 1920, speed 3643.56 words/sec, time elapsed 1623.26 sec\n",
            "epoch 9, iter 4070, avg. loss 0.17, avg. ppl 1.01 cum. examples 2240, speed 3380.20 words/sec, time elapsed 1626.04 sec\n",
            "epoch 9, iter 4080, avg. loss 0.17, avg. ppl 1.01 cum. examples 2560, speed 3615.38 words/sec, time elapsed 1628.59 sec\n",
            "epoch 9, iter 4090, avg. loss 0.14, avg. ppl 1.00 cum. examples 2880, speed 3346.70 words/sec, time elapsed 1631.36 sec\n",
            "epoch 9, iter 4100, avg. loss 0.11, avg. ppl 1.00 cum. examples 3200, speed 3785.74 words/sec, time elapsed 1633.73 sec\n",
            "epoch 9, iter 4110, avg. loss 0.30, avg. ppl 1.01 cum. examples 3520, speed 3160.58 words/sec, time elapsed 1636.72 sec\n",
            "epoch 9, iter 4120, avg. loss 0.19, avg. ppl 1.01 cum. examples 3840, speed 3574.45 words/sec, time elapsed 1639.42 sec\n",
            "epoch 9, iter 4130, avg. loss 0.33, avg. ppl 1.01 cum. examples 4160, speed 3401.34 words/sec, time elapsed 1641.97 sec\n",
            "epoch 9, iter 4140, avg. loss 0.19, avg. ppl 1.01 cum. examples 4480, speed 3329.41 words/sec, time elapsed 1644.72 sec\n",
            "epoch 9, iter 4150, avg. loss 0.13, avg. ppl 1.00 cum. examples 4800, speed 3493.69 words/sec, time elapsed 1647.44 sec\n",
            "epoch 9, iter 4160, avg. loss 0.13, avg. ppl 1.00 cum. examples 5120, speed 3433.18 words/sec, time elapsed 1650.09 sec\n",
            "epoch 9, iter 4170, avg. loss 0.09, avg. ppl 1.00 cum. examples 5440, speed 3292.41 words/sec, time elapsed 1652.82 sec\n",
            "epoch 9, iter 4180, avg. loss 0.06, avg. ppl 1.00 cum. examples 5760, speed 3320.40 words/sec, time elapsed 1655.66 sec\n",
            "epoch 10, iter 4190, avg. loss 0.11, avg. ppl 1.00 cum. examples 6055, speed 3598.88 words/sec, time elapsed 1657.97 sec\n",
            "epoch 10, iter 4200, avg. loss 0.12, avg. ppl 1.00 cum. examples 6375, speed 3687.98 words/sec, time elapsed 1660.38 sec\n",
            "epoch 10, iter 4200, cum. loss 0.15, cum. ppl 1.01 cum. examples 6375\n",
            "begin validation ...\n",
            "validation: iter 4200, dev. ppl 1.002962\n",
            "save currently the best model to [model.bin]\n",
            "save model parameters to [model.bin]\n",
            "epoch 10, iter 4210, avg. loss 0.08, avg. ppl 1.00 cum. examples 320, speed 293.70 words/sec, time elapsed 1691.47 sec\n",
            "epoch 10, iter 4220, avg. loss 0.04, avg. ppl 1.00 cum. examples 640, speed 3488.37 words/sec, time elapsed 1694.11 sec\n",
            "epoch 10, iter 4230, avg. loss 0.10, avg. ppl 1.00 cum. examples 960, speed 3713.32 words/sec, time elapsed 1696.56 sec\n",
            "epoch 10, iter 4240, avg. loss 0.10, avg. ppl 1.00 cum. examples 1280, speed 3497.27 words/sec, time elapsed 1699.22 sec\n",
            "epoch 10, iter 4250, avg. loss 0.12, avg. ppl 1.00 cum. examples 1600, speed 3426.25 words/sec, time elapsed 1701.91 sec\n",
            "epoch 10, iter 4260, avg. loss 0.06, avg. ppl 1.00 cum. examples 1920, speed 3613.57 words/sec, time elapsed 1704.45 sec\n",
            "epoch 10, iter 4270, avg. loss 0.08, avg. ppl 1.00 cum. examples 2240, speed 3617.76 words/sec, time elapsed 1706.97 sec\n",
            "epoch 10, iter 4280, avg. loss 0.04, avg. ppl 1.00 cum. examples 2560, speed 3685.15 words/sec, time elapsed 1709.56 sec\n",
            "epoch 10, iter 4290, avg. loss 0.11, avg. ppl 1.00 cum. examples 2880, speed 3717.57 words/sec, time elapsed 1712.10 sec\n",
            "epoch 10, iter 4300, avg. loss 0.06, avg. ppl 1.00 cum. examples 3200, speed 3463.38 words/sec, time elapsed 1714.84 sec\n",
            "epoch 10, iter 4310, avg. loss 0.09, avg. ppl 1.00 cum. examples 3520, speed 3465.13 words/sec, time elapsed 1717.45 sec\n",
            "epoch 10, iter 4320, avg. loss 0.09, avg. ppl 1.00 cum. examples 3840, speed 3905.19 words/sec, time elapsed 1719.85 sec\n",
            "epoch 10, iter 4330, avg. loss 0.08, avg. ppl 1.00 cum. examples 4160, speed 3441.36 words/sec, time elapsed 1722.55 sec\n",
            "epoch 10, iter 4340, avg. loss 0.07, avg. ppl 1.00 cum. examples 4480, speed 3754.22 words/sec, time elapsed 1724.96 sec\n",
            "epoch 10, iter 4350, avg. loss 0.05, avg. ppl 1.00 cum. examples 4800, speed 3848.56 words/sec, time elapsed 1727.30 sec\n",
            "epoch 10, iter 4360, avg. loss 0.18, avg. ppl 1.01 cum. examples 5120, speed 3518.42 words/sec, time elapsed 1729.96 sec\n",
            "epoch 10, iter 4370, avg. loss 0.10, avg. ppl 1.00 cum. examples 5440, speed 3753.72 words/sec, time elapsed 1732.47 sec\n",
            "epoch 10, iter 4380, avg. loss 0.11, avg. ppl 1.00 cum. examples 5760, speed 3651.85 words/sec, time elapsed 1734.85 sec\n",
            "epoch 10, iter 4390, avg. loss 0.29, avg. ppl 1.01 cum. examples 6080, speed 2929.71 words/sec, time elapsed 1738.04 sec\n",
            "epoch 10, iter 4400, avg. loss 0.27, avg. ppl 1.01 cum. examples 6400, speed 3448.45 words/sec, time elapsed 1740.82 sec\n",
            "epoch 10, iter 4400, cum. loss 0.11, cum. ppl 1.00 cum. examples 6400\n",
            "begin validation ...\n",
            "validation: iter 4400, dev. ppl 1.003023\n",
            "hit patience 1\n",
            "hit #4 trial\n",
            "load previously best model and decay learning rate to 0.000031\n",
            "restore parameters of the optimizers\n",
            "epoch 10, iter 4410, avg. loss 0.17, avg. ppl 1.01 cum. examples 320, speed 333.04 words/sec, time elapsed 1769.18 sec\n",
            "epoch 10, iter 4420, avg. loss 0.10, avg. ppl 1.00 cum. examples 640, speed 3691.47 words/sec, time elapsed 1771.68 sec\n",
            "epoch 10, iter 4430, avg. loss 0.09, avg. ppl 1.00 cum. examples 960, speed 3235.48 words/sec, time elapsed 1774.67 sec\n",
            "epoch 10, iter 4440, avg. loss 0.06, avg. ppl 1.00 cum. examples 1280, speed 3621.57 words/sec, time elapsed 1777.17 sec\n",
            "epoch 10, iter 4450, avg. loss 0.05, avg. ppl 1.00 cum. examples 1600, speed 3621.38 words/sec, time elapsed 1779.71 sec\n",
            "epoch 10, iter 4460, avg. loss 0.06, avg. ppl 1.00 cum. examples 1920, speed 3616.21 words/sec, time elapsed 1782.20 sec\n",
            "epoch 10, iter 4470, avg. loss 0.10, avg. ppl 1.00 cum. examples 2240, speed 3567.26 words/sec, time elapsed 1784.78 sec\n",
            "epoch 10, iter 4480, avg. loss 0.07, avg. ppl 1.00 cum. examples 2560, speed 3276.88 words/sec, time elapsed 1787.61 sec\n",
            "epoch 10, iter 4490, avg. loss 0.09, avg. ppl 1.00 cum. examples 2880, speed 3658.91 words/sec, time elapsed 1790.18 sec\n",
            "epoch 10, iter 4500, avg. loss 0.05, avg. ppl 1.00 cum. examples 3200, speed 3602.23 words/sec, time elapsed 1792.74 sec\n",
            "epoch 10, iter 4510, avg. loss 0.12, avg. ppl 1.00 cum. examples 3520, speed 3585.28 words/sec, time elapsed 1795.31 sec\n",
            "epoch 10, iter 4520, avg. loss 0.08, avg. ppl 1.00 cum. examples 3840, speed 3670.59 words/sec, time elapsed 1797.81 sec\n",
            "epoch 10, iter 4530, avg. loss 0.57, avg. ppl 1.02 cum. examples 4160, speed 3433.13 words/sec, time elapsed 1800.45 sec\n",
            "epoch 10, iter 4540, avg. loss 0.10, avg. ppl 1.00 cum. examples 4480, speed 3347.58 words/sec, time elapsed 1803.27 sec\n",
            "epoch 10, iter 4550, avg. loss 0.17, avg. ppl 1.01 cum. examples 4800, speed 3793.51 words/sec, time elapsed 1805.70 sec\n",
            "epoch 10, iter 4560, avg. loss 0.11, avg. ppl 1.00 cum. examples 5120, speed 3170.06 words/sec, time elapsed 1808.55 sec\n",
            "epoch 10, iter 4570, avg. loss 0.14, avg. ppl 1.01 cum. examples 5440, speed 3226.64 words/sec, time elapsed 1811.32 sec\n",
            "epoch 10, iter 4580, avg. loss 0.06, avg. ppl 1.00 cum. examples 5760, speed 3625.53 words/sec, time elapsed 1813.87 sec\n",
            "epoch 10, iter 4590, avg. loss 0.09, avg. ppl 1.00 cum. examples 6080, speed 3605.10 words/sec, time elapsed 1816.26 sec\n",
            "epoch 10, iter 4600, avg. loss 0.08, avg. ppl 1.00 cum. examples 6400, speed 3590.19 words/sec, time elapsed 1818.87 sec\n",
            "epoch 10, iter 4600, cum. loss 0.12, cum. ppl 1.00 cum. examples 6400\n",
            "begin validation ...\n",
            "validation: iter 4600, dev. ppl 1.002501\n",
            "save currently the best model to [model.bin]\n",
            "save model parameters to [model.bin]\n",
            "epoch 10, iter 4610, avg. loss 0.04, avg. ppl 1.00 cum. examples 320, speed 276.26 words/sec, time elapsed 1852.56 sec\n",
            "epoch 10, iter 4620, avg. loss 0.14, avg. ppl 1.00 cum. examples 640, speed 3565.80 words/sec, time elapsed 1855.23 sec\n",
            "epoch 10, iter 4630, avg. loss 0.10, avg. ppl 1.00 cum. examples 960, speed 3239.93 words/sec, time elapsed 1858.00 sec\n",
            "epoch 10, iter 4640, avg. loss 0.24, avg. ppl 1.01 cum. examples 1280, speed 3704.37 words/sec, time elapsed 1860.56 sec\n",
            "epoch 10, iter 4650, avg. loss 0.10, avg. ppl 1.00 cum. examples 1575, speed 3456.45 words/sec, time elapsed 1863.00 sec\n",
            "epoch 11, iter 4660, avg. loss 0.09, avg. ppl 1.00 cum. examples 1895, speed 3433.12 words/sec, time elapsed 1865.65 sec\n",
            "epoch 11, iter 4670, avg. loss 0.09, avg. ppl 1.00 cum. examples 2215, speed 3646.67 words/sec, time elapsed 1868.15 sec\n",
            "epoch 11, iter 4680, avg. loss 0.06, avg. ppl 1.00 cum. examples 2535, speed 3334.96 words/sec, time elapsed 1870.89 sec\n",
            "epoch 11, iter 4690, avg. loss 0.06, avg. ppl 1.00 cum. examples 2855, speed 3637.67 words/sec, time elapsed 1873.44 sec\n",
            "epoch 11, iter 4700, avg. loss 0.09, avg. ppl 1.00 cum. examples 3175, speed 3700.02 words/sec, time elapsed 1875.95 sec\n",
            "epoch 11, iter 4710, avg. loss 0.10, avg. ppl 1.00 cum. examples 3495, speed 3619.55 words/sec, time elapsed 1878.48 sec\n",
            "epoch 11, iter 4720, avg. loss 0.06, avg. ppl 1.00 cum. examples 3815, speed 3428.65 words/sec, time elapsed 1881.29 sec\n",
            "epoch 11, iter 4730, avg. loss 0.04, avg. ppl 1.00 cum. examples 4135, speed 3735.79 words/sec, time elapsed 1883.81 sec\n",
            "epoch 11, iter 4740, avg. loss 0.13, avg. ppl 1.00 cum. examples 4455, speed 3140.76 words/sec, time elapsed 1886.85 sec\n",
            "epoch 11, iter 4750, avg. loss 0.13, avg. ppl 1.00 cum. examples 4775, speed 3733.11 words/sec, time elapsed 1889.43 sec\n",
            "epoch 11, iter 4760, avg. loss 0.02, avg. ppl 1.00 cum. examples 5095, speed 3404.54 words/sec, time elapsed 1892.25 sec\n",
            "epoch 11, iter 4770, avg. loss 0.06, avg. ppl 1.00 cum. examples 5415, speed 3636.40 words/sec, time elapsed 1894.78 sec\n",
            "epoch 11, iter 4780, avg. loss 0.12, avg. ppl 1.00 cum. examples 5735, speed 3843.39 words/sec, time elapsed 1897.26 sec\n",
            "epoch 11, iter 4790, avg. loss 0.11, avg. ppl 1.00 cum. examples 6055, speed 3703.38 words/sec, time elapsed 1899.83 sec\n",
            "epoch 11, iter 4800, avg. loss 0.15, avg. ppl 1.01 cum. examples 6375, speed 3696.31 words/sec, time elapsed 1902.26 sec\n",
            "epoch 11, iter 4800, cum. loss 0.10, cum. ppl 1.00 cum. examples 6375\n",
            "begin validation ...\n",
            "validation: iter 4800, dev. ppl 1.002196\n",
            "save currently the best model to [model.bin]\n",
            "save model parameters to [model.bin]\n",
            "epoch 11, iter 4810, avg. loss 0.11, avg. ppl 1.00 cum. examples 320, speed 284.14 words/sec, time elapsed 1932.94 sec\n",
            "epoch 11, iter 4820, avg. loss 0.06, avg. ppl 1.00 cum. examples 640, speed 3333.65 words/sec, time elapsed 1935.82 sec\n",
            "epoch 11, iter 4830, avg. loss 0.05, avg. ppl 1.00 cum. examples 960, speed 3864.43 words/sec, time elapsed 1938.17 sec\n",
            "epoch 11, iter 4840, avg. loss 0.07, avg. ppl 1.00 cum. examples 1280, speed 3229.26 words/sec, time elapsed 1940.98 sec\n",
            "epoch 11, iter 4850, avg. loss 0.08, avg. ppl 1.00 cum. examples 1600, speed 3300.50 words/sec, time elapsed 1943.82 sec\n",
            "epoch 11, iter 4860, avg. loss 0.06, avg. ppl 1.00 cum. examples 1920, speed 3667.90 words/sec, time elapsed 1946.35 sec\n",
            "epoch 11, iter 4870, avg. loss 0.14, avg. ppl 1.00 cum. examples 2240, speed 3373.04 words/sec, time elapsed 1949.22 sec\n",
            "epoch 11, iter 4880, avg. loss 0.06, avg. ppl 1.00 cum. examples 2560, speed 3727.45 words/sec, time elapsed 1951.67 sec\n",
            "epoch 11, iter 4890, avg. loss 0.16, avg. ppl 1.01 cum. examples 2880, speed 3299.62 words/sec, time elapsed 1954.37 sec\n",
            "epoch 11, iter 4900, avg. loss 0.08, avg. ppl 1.00 cum. examples 3200, speed 3550.44 words/sec, time elapsed 1957.10 sec\n",
            "epoch 11, iter 4910, avg. loss 0.14, avg. ppl 1.00 cum. examples 3520, speed 3367.27 words/sec, time elapsed 1959.82 sec\n",
            "epoch 11, iter 4920, avg. loss 0.09, avg. ppl 1.00 cum. examples 3840, speed 3771.66 words/sec, time elapsed 1962.21 sec\n",
            "epoch 11, iter 4930, avg. loss 0.12, avg. ppl 1.00 cum. examples 4160, speed 3289.58 words/sec, time elapsed 1964.96 sec\n",
            "epoch 11, iter 4940, avg. loss 0.09, avg. ppl 1.00 cum. examples 4480, speed 3374.47 words/sec, time elapsed 1967.59 sec\n",
            "epoch 11, iter 4950, avg. loss 0.06, avg. ppl 1.00 cum. examples 4800, speed 3804.97 words/sec, time elapsed 1970.02 sec\n",
            "epoch 11, iter 4960, avg. loss 0.22, avg. ppl 1.01 cum. examples 5120, speed 3687.25 words/sec, time elapsed 1972.50 sec\n",
            "epoch 11, iter 4970, avg. loss 0.05, avg. ppl 1.00 cum. examples 5440, speed 3627.32 words/sec, time elapsed 1975.08 sec\n",
            "epoch 11, iter 4980, avg. loss 0.14, avg. ppl 1.01 cum. examples 5760, speed 3753.81 words/sec, time elapsed 1977.50 sec\n",
            "epoch 11, iter 4990, avg. loss 0.09, avg. ppl 1.00 cum. examples 6080, speed 3668.17 words/sec, time elapsed 1980.03 sec\n",
            "epoch 11, iter 5000, avg. loss 0.05, avg. ppl 1.00 cum. examples 6400, speed 3822.80 words/sec, time elapsed 1982.39 sec\n",
            "epoch 11, iter 5000, cum. loss 0.10, cum. ppl 1.00 cum. examples 6400\n",
            "begin validation ...\n",
            "validation: iter 5000, dev. ppl 1.001914\n",
            "save currently the best model to [model.bin]\n",
            "save model parameters to [model.bin]\n",
            "epoch 11, iter 5010, avg. loss 0.03, avg. ppl 1.00 cum. examples 320, speed 288.18 words/sec, time elapsed 2013.33 sec\n",
            "epoch 11, iter 5020, avg. loss 0.07, avg. ppl 1.00 cum. examples 640, speed 3819.72 words/sec, time elapsed 2015.74 sec\n",
            "epoch 11, iter 5030, avg. loss 0.04, avg. ppl 1.00 cum. examples 960, speed 3650.78 words/sec, time elapsed 2018.36 sec\n",
            "epoch 11, iter 5040, avg. loss 0.06, avg. ppl 1.00 cum. examples 1280, speed 3276.55 words/sec, time elapsed 2021.06 sec\n",
            "epoch 11, iter 5050, avg. loss 0.09, avg. ppl 1.00 cum. examples 1600, speed 3677.47 words/sec, time elapsed 2023.58 sec\n",
            "epoch 11, iter 5060, avg. loss 0.05, avg. ppl 1.00 cum. examples 1920, speed 3516.89 words/sec, time elapsed 2026.11 sec\n",
            "epoch 11, iter 5070, avg. loss 0.06, avg. ppl 1.00 cum. examples 2240, speed 3626.19 words/sec, time elapsed 2028.61 sec\n",
            "epoch 11, iter 5080, avg. loss 0.12, avg. ppl 1.00 cum. examples 2560, speed 3043.32 words/sec, time elapsed 2031.51 sec\n",
            "epoch 11, iter 5090, avg. loss 0.05, avg. ppl 1.00 cum. examples 2880, speed 3814.74 words/sec, time elapsed 2033.87 sec\n",
            "epoch 11, iter 5100, avg. loss 0.08, avg. ppl 1.00 cum. examples 3200, speed 3482.71 words/sec, time elapsed 2036.57 sec\n",
            "epoch 11, iter 5110, avg. loss 0.44, avg. ppl 1.02 cum. examples 3520, speed 3580.65 words/sec, time elapsed 2039.18 sec\n",
            "epoch 12, iter 5120, avg. loss 0.05, avg. ppl 1.00 cum. examples 3815, speed 3199.85 words/sec, time elapsed 2041.74 sec\n",
            "epoch 12, iter 5130, avg. loss 0.07, avg. ppl 1.00 cum. examples 4135, speed 3788.36 words/sec, time elapsed 2044.24 sec\n",
            "epoch 12, iter 5140, avg. loss 0.03, avg. ppl 1.00 cum. examples 4455, speed 3626.20 words/sec, time elapsed 2046.74 sec\n",
            "epoch 12, iter 5150, avg. loss 0.12, avg. ppl 1.00 cum. examples 4775, speed 3535.05 words/sec, time elapsed 2049.48 sec\n",
            "epoch 12, iter 5160, avg. loss 0.05, avg. ppl 1.00 cum. examples 5095, speed 3269.32 words/sec, time elapsed 2052.48 sec\n",
            "epoch 12, iter 5170, avg. loss 0.50, avg. ppl 1.02 cum. examples 5415, speed 3526.58 words/sec, time elapsed 2055.17 sec\n",
            "epoch 12, iter 5180, avg. loss 0.09, avg. ppl 1.00 cum. examples 5735, speed 3335.94 words/sec, time elapsed 2057.94 sec\n",
            "epoch 12, iter 5190, avg. loss 0.09, avg. ppl 1.00 cum. examples 6055, speed 3182.33 words/sec, time elapsed 2060.82 sec\n",
            "epoch 12, iter 5200, avg. loss 0.06, avg. ppl 1.00 cum. examples 6375, speed 3370.49 words/sec, time elapsed 2063.49 sec\n",
            "epoch 12, iter 5200, cum. loss 0.11, cum. ppl 1.00 cum. examples 6375\n",
            "begin validation ...\n",
            "validation: iter 5200, dev. ppl 1.001820\n",
            "save currently the best model to [model.bin]\n",
            "save model parameters to [model.bin]\n",
            "epoch 12, iter 5210, avg. loss 0.03, avg. ppl 1.00 cum. examples 320, speed 290.72 words/sec, time elapsed 2094.49 sec\n",
            "epoch 12, iter 5220, avg. loss 0.14, avg. ppl 1.00 cum. examples 640, speed 3654.13 words/sec, time elapsed 2097.14 sec\n",
            "epoch 12, iter 5230, avg. loss 0.07, avg. ppl 1.00 cum. examples 960, speed 3575.17 words/sec, time elapsed 2099.74 sec\n",
            "epoch 12, iter 5240, avg. loss 0.03, avg. ppl 1.00 cum. examples 1280, speed 3598.06 words/sec, time elapsed 2102.23 sec\n",
            "epoch 12, iter 5250, avg. loss 0.04, avg. ppl 1.00 cum. examples 1600, speed 3371.05 words/sec, time elapsed 2105.03 sec\n",
            "epoch 12, iter 5260, avg. loss 0.03, avg. ppl 1.00 cum. examples 1920, speed 3475.06 words/sec, time elapsed 2107.52 sec\n",
            "epoch 12, iter 5270, avg. loss 0.04, avg. ppl 1.00 cum. examples 2240, speed 3495.19 words/sec, time elapsed 2110.10 sec\n",
            "epoch 12, iter 5280, avg. loss 0.05, avg. ppl 1.00 cum. examples 2560, speed 3545.42 words/sec, time elapsed 2112.67 sec\n",
            "epoch 12, iter 5290, avg. loss 0.08, avg. ppl 1.00 cum. examples 2880, speed 3380.15 words/sec, time elapsed 2115.35 sec\n",
            "epoch 12, iter 5300, avg. loss 0.12, avg. ppl 1.00 cum. examples 3200, speed 3367.55 words/sec, time elapsed 2118.05 sec\n",
            "epoch 12, iter 5310, avg. loss 0.05, avg. ppl 1.00 cum. examples 3520, speed 3639.61 words/sec, time elapsed 2120.64 sec\n",
            "epoch 12, iter 5320, avg. loss 0.05, avg. ppl 1.00 cum. examples 3840, speed 3559.00 words/sec, time elapsed 2123.22 sec\n",
            "epoch 12, iter 5330, avg. loss 0.13, avg. ppl 1.00 cum. examples 4160, speed 3126.11 words/sec, time elapsed 2126.02 sec\n",
            "epoch 12, iter 5340, avg. loss 0.07, avg. ppl 1.00 cum. examples 4480, speed 3361.60 words/sec, time elapsed 2128.74 sec\n",
            "epoch 12, iter 5350, avg. loss 0.15, avg. ppl 1.01 cum. examples 4800, speed 3643.41 words/sec, time elapsed 2131.35 sec\n",
            "epoch 12, iter 5360, avg. loss 0.09, avg. ppl 1.00 cum. examples 5120, speed 3687.12 words/sec, time elapsed 2133.78 sec\n",
            "epoch 12, iter 5370, avg. loss 0.05, avg. ppl 1.00 cum. examples 5440, speed 3685.84 words/sec, time elapsed 2136.26 sec\n",
            "epoch 12, iter 5380, avg. loss 0.08, avg. ppl 1.00 cum. examples 5760, speed 3514.56 words/sec, time elapsed 2138.94 sec\n",
            "epoch 12, iter 5390, avg. loss 0.05, avg. ppl 1.00 cum. examples 6080, speed 3632.77 words/sec, time elapsed 2141.50 sec\n",
            "epoch 12, iter 5400, avg. loss 0.06, avg. ppl 1.00 cum. examples 6400, speed 3570.92 words/sec, time elapsed 2144.06 sec\n",
            "epoch 12, iter 5400, cum. loss 0.07, cum. ppl 1.00 cum. examples 6400\n",
            "begin validation ...\n",
            "validation: iter 5400, dev. ppl 1.001586\n",
            "save currently the best model to [model.bin]\n",
            "save model parameters to [model.bin]\n",
            "epoch 12, iter 5410, avg. loss 0.04, avg. ppl 1.00 cum. examples 320, speed 271.03 words/sec, time elapsed 2178.65 sec\n",
            "epoch 12, iter 5420, avg. loss 0.06, avg. ppl 1.00 cum. examples 640, speed 3388.13 words/sec, time elapsed 2181.42 sec\n",
            "epoch 12, iter 5430, avg. loss 0.06, avg. ppl 1.00 cum. examples 960, speed 3489.03 words/sec, time elapsed 2184.08 sec\n",
            "epoch 12, iter 5440, avg. loss 0.04, avg. ppl 1.00 cum. examples 1280, speed 3337.01 words/sec, time elapsed 2186.81 sec\n",
            "epoch 12, iter 5450, avg. loss 0.07, avg. ppl 1.00 cum. examples 1600, speed 3662.54 words/sec, time elapsed 2189.30 sec\n",
            "epoch 12, iter 5460, avg. loss 0.09, avg. ppl 1.00 cum. examples 1920, speed 3629.58 words/sec, time elapsed 2191.79 sec\n",
            "epoch 12, iter 5470, avg. loss 0.08, avg. ppl 1.00 cum. examples 2240, speed 3522.38 words/sec, time elapsed 2194.36 sec\n",
            "epoch 12, iter 5480, avg. loss 0.12, avg. ppl 1.00 cum. examples 2560, speed 3358.97 words/sec, time elapsed 2197.09 sec\n",
            "epoch 12, iter 5490, avg. loss 0.06, avg. ppl 1.00 cum. examples 2880, speed 3606.41 words/sec, time elapsed 2199.63 sec\n",
            "epoch 12, iter 5500, avg. loss 0.04, avg. ppl 1.00 cum. examples 3200, speed 3705.43 words/sec, time elapsed 2202.17 sec\n",
            "epoch 12, iter 5510, avg. loss 0.07, avg. ppl 1.00 cum. examples 3520, speed 3630.85 words/sec, time elapsed 2204.79 sec\n",
            "epoch 12, iter 5520, avg. loss 0.03, avg. ppl 1.00 cum. examples 3840, speed 3620.21 words/sec, time elapsed 2207.23 sec\n",
            "epoch 12, iter 5530, avg. loss 0.04, avg. ppl 1.00 cum. examples 4160, speed 3861.43 words/sec, time elapsed 2209.71 sec\n",
            "epoch 12, iter 5540, avg. loss 0.10, avg. ppl 1.00 cum. examples 4480, speed 3289.60 words/sec, time elapsed 2212.37 sec\n",
            "epoch 12, iter 5550, avg. loss 0.09, avg. ppl 1.00 cum. examples 4800, speed 3862.48 words/sec, time elapsed 2214.76 sec\n",
            "epoch 12, iter 5560, avg. loss 0.02, avg. ppl 1.00 cum. examples 5120, speed 3725.76 words/sec, time elapsed 2217.35 sec\n",
            "epoch 12, iter 5570, avg. loss 0.03, avg. ppl 1.00 cum. examples 5440, speed 3811.73 words/sec, time elapsed 2219.80 sec\n",
            "epoch 12, iter 5580, avg. loss 0.02, avg. ppl 1.00 cum. examples 5735, speed 3585.08 words/sec, time elapsed 2222.12 sec\n",
            "epoch 13, iter 5590, avg. loss 0.03, avg. ppl 1.00 cum. examples 6055, speed 3536.42 words/sec, time elapsed 2224.76 sec\n",
            "epoch 13, iter 5600, avg. loss 0.09, avg. ppl 1.00 cum. examples 6375, speed 3363.29 words/sec, time elapsed 2227.52 sec\n",
            "epoch 13, iter 5600, cum. loss 0.06, cum. ppl 1.00 cum. examples 6375\n",
            "begin validation ...\n",
            "validation: iter 5600, dev. ppl 1.001488\n",
            "save currently the best model to [model.bin]\n",
            "save model parameters to [model.bin]\n",
            "epoch 13, iter 5610, avg. loss 0.08, avg. ppl 1.00 cum. examples 320, speed 274.70 words/sec, time elapsed 2262.39 sec\n",
            "epoch 13, iter 5620, avg. loss 0.05, avg. ppl 1.00 cum. examples 640, speed 3735.89 words/sec, time elapsed 2264.93 sec\n",
            "epoch 13, iter 5630, avg. loss 0.07, avg. ppl 1.00 cum. examples 960, speed 3780.91 words/sec, time elapsed 2267.38 sec\n",
            "epoch 13, iter 5640, avg. loss 0.09, avg. ppl 1.00 cum. examples 1280, speed 3513.20 words/sec, time elapsed 2269.87 sec\n",
            "epoch 13, iter 5650, avg. loss 0.15, avg. ppl 1.01 cum. examples 1600, speed 3300.63 words/sec, time elapsed 2272.65 sec\n",
            "epoch 13, iter 5660, avg. loss 0.04, avg. ppl 1.00 cum. examples 1920, speed 3600.10 words/sec, time elapsed 2275.37 sec\n",
            "epoch 13, iter 5670, avg. loss 0.02, avg. ppl 1.00 cum. examples 2240, speed 3765.69 words/sec, time elapsed 2277.83 sec\n",
            "epoch 13, iter 5680, avg. loss 0.05, avg. ppl 1.00 cum. examples 2560, speed 3656.36 words/sec, time elapsed 2280.48 sec\n",
            "epoch 13, iter 5690, avg. loss 0.05, avg. ppl 1.00 cum. examples 2880, speed 3360.51 words/sec, time elapsed 2283.28 sec\n",
            "epoch 13, iter 5700, avg. loss 0.04, avg. ppl 1.00 cum. examples 3200, speed 3850.01 words/sec, time elapsed 2285.65 sec\n",
            "epoch 13, iter 5710, avg. loss 0.09, avg. ppl 1.00 cum. examples 3520, speed 3350.54 words/sec, time elapsed 2288.34 sec\n",
            "epoch 13, iter 5720, avg. loss 0.04, avg. ppl 1.00 cum. examples 3840, speed 3682.54 words/sec, time elapsed 2290.96 sec\n",
            "epoch 13, iter 5730, avg. loss 0.02, avg. ppl 1.00 cum. examples 4160, speed 3794.84 words/sec, time elapsed 2293.49 sec\n",
            "epoch 13, iter 5740, avg. loss 0.03, avg. ppl 1.00 cum. examples 4480, speed 3711.99 words/sec, time elapsed 2296.04 sec\n",
            "epoch 13, iter 5750, avg. loss 0.04, avg. ppl 1.00 cum. examples 4800, speed 3575.15 words/sec, time elapsed 2298.61 sec\n",
            "epoch 13, iter 5760, avg. loss 0.14, avg. ppl 1.00 cum. examples 5120, speed 3314.47 words/sec, time elapsed 2301.44 sec\n",
            "epoch 13, iter 5770, avg. loss 0.03, avg. ppl 1.00 cum. examples 5440, speed 3639.43 words/sec, time elapsed 2303.97 sec\n",
            "epoch 13, iter 5780, avg. loss 0.01, avg. ppl 1.00 cum. examples 5760, speed 3828.49 words/sec, time elapsed 2306.25 sec\n",
            "epoch 13, iter 5790, avg. loss 0.02, avg. ppl 1.00 cum. examples 6080, speed 3470.33 words/sec, time elapsed 2308.85 sec\n",
            "epoch 13, iter 5800, avg. loss 0.02, avg. ppl 1.00 cum. examples 6400, speed 3337.14 words/sec, time elapsed 2311.55 sec\n",
            "epoch 13, iter 5800, cum. loss 0.05, cum. ppl 1.00 cum. examples 6400\n",
            "begin validation ...\n",
            "validation: iter 5800, dev. ppl 1.001420\n",
            "save currently the best model to [model.bin]\n",
            "save model parameters to [model.bin]\n",
            "epoch 13, iter 5810, avg. loss 0.02, avg. ppl 1.00 cum. examples 320, speed 300.85 words/sec, time elapsed 2342.39 sec\n",
            "epoch 13, iter 5820, avg. loss 0.05, avg. ppl 1.00 cum. examples 640, speed 3692.67 words/sec, time elapsed 2344.77 sec\n",
            "epoch 13, iter 5830, avg. loss 0.07, avg. ppl 1.00 cum. examples 960, speed 3679.11 words/sec, time elapsed 2347.39 sec\n",
            "epoch 13, iter 5840, avg. loss 0.04, avg. ppl 1.00 cum. examples 1280, speed 3660.16 words/sec, time elapsed 2349.72 sec\n",
            "epoch 13, iter 5850, avg. loss 0.04, avg. ppl 1.00 cum. examples 1600, speed 3355.39 words/sec, time elapsed 2352.45 sec\n",
            "epoch 13, iter 5860, avg. loss 0.03, avg. ppl 1.00 cum. examples 1920, speed 3684.13 words/sec, time elapsed 2354.94 sec\n",
            "epoch 13, iter 5870, avg. loss 0.50, avg. ppl 1.02 cum. examples 2240, speed 3025.40 words/sec, time elapsed 2357.90 sec\n",
            "epoch 13, iter 5880, avg. loss 0.12, avg. ppl 1.00 cum. examples 2560, speed 3198.69 words/sec, time elapsed 2360.63 sec\n",
            "epoch 13, iter 5890, avg. loss 0.07, avg. ppl 1.00 cum. examples 2880, speed 3329.80 words/sec, time elapsed 2363.47 sec\n",
            "epoch 13, iter 5900, avg. loss 0.06, avg. ppl 1.00 cum. examples 3200, speed 3565.33 words/sec, time elapsed 2366.09 sec\n",
            "epoch 13, iter 5910, avg. loss 0.04, avg. ppl 1.00 cum. examples 3520, speed 3267.11 words/sec, time elapsed 2368.97 sec\n",
            "epoch 13, iter 5920, avg. loss 0.15, avg. ppl 1.01 cum. examples 3840, speed 3306.06 words/sec, time elapsed 2371.80 sec\n",
            "epoch 13, iter 5930, avg. loss 0.05, avg. ppl 1.00 cum. examples 4160, speed 3425.71 words/sec, time elapsed 2374.46 sec\n",
            "epoch 13, iter 5940, avg. loss 0.03, avg. ppl 1.00 cum. examples 4480, speed 3499.77 words/sec, time elapsed 2377.14 sec\n",
            "epoch 13, iter 5950, avg. loss 0.06, avg. ppl 1.00 cum. examples 4800, speed 3513.92 words/sec, time elapsed 2379.88 sec\n",
            "epoch 13, iter 5960, avg. loss 0.04, avg. ppl 1.00 cum. examples 5120, speed 3747.43 words/sec, time elapsed 2382.42 sec\n",
            "epoch 13, iter 5970, avg. loss 0.03, avg. ppl 1.00 cum. examples 5440, speed 3526.92 words/sec, time elapsed 2385.01 sec\n",
            "epoch 13, iter 5980, avg. loss 0.07, avg. ppl 1.00 cum. examples 5760, speed 3996.15 words/sec, time elapsed 2387.33 sec\n",
            "epoch 13, iter 5990, avg. loss 0.04, avg. ppl 1.00 cum. examples 6080, speed 3662.97 words/sec, time elapsed 2389.91 sec\n",
            "epoch 13, iter 6000, avg. loss 0.02, avg. ppl 1.00 cum. examples 6400, speed 3485.75 words/sec, time elapsed 2392.40 sec\n",
            "epoch 13, iter 6000, cum. loss 0.08, cum. ppl 1.00 cum. examples 6400\n",
            "begin validation ...\n",
            "validation: iter 6000, dev. ppl 1.001339\n",
            "save currently the best model to [model.bin]\n",
            "save model parameters to [model.bin]\n",
            "epoch 13, iter 6010, avg. loss 0.04, avg. ppl 1.00 cum. examples 320, speed 251.35 words/sec, time elapsed 2427.51 sec\n",
            "epoch 13, iter 6020, avg. loss 0.02, avg. ppl 1.00 cum. examples 640, speed 3573.09 words/sec, time elapsed 2430.03 sec\n",
            "epoch 13, iter 6030, avg. loss 0.11, avg. ppl 1.00 cum. examples 960, speed 3140.69 words/sec, time elapsed 2432.92 sec\n",
            "epoch 13, iter 6040, avg. loss 0.10, avg. ppl 1.00 cum. examples 1280, speed 3369.12 words/sec, time elapsed 2435.54 sec\n",
            "epoch 14, iter 6050, avg. loss 0.09, avg. ppl 1.00 cum. examples 1575, speed 3540.25 words/sec, time elapsed 2437.94 sec\n",
            "epoch 14, iter 6060, avg. loss 0.04, avg. ppl 1.00 cum. examples 1895, speed 3287.93 words/sec, time elapsed 2440.81 sec\n",
            "epoch 14, iter 6070, avg. loss 0.03, avg. ppl 1.00 cum. examples 2215, speed 3520.38 words/sec, time elapsed 2443.46 sec\n",
            "epoch 14, iter 6080, avg. loss 0.08, avg. ppl 1.00 cum. examples 2535, speed 3304.32 words/sec, time elapsed 2446.33 sec\n",
            "epoch 14, iter 6090, avg. loss 0.08, avg. ppl 1.00 cum. examples 2855, speed 3416.71 words/sec, time elapsed 2449.04 sec\n",
            "epoch 14, iter 6100, avg. loss 0.06, avg. ppl 1.00 cum. examples 3175, speed 3446.26 words/sec, time elapsed 2451.64 sec\n",
            "epoch 14, iter 6110, avg. loss 0.05, avg. ppl 1.00 cum. examples 3495, speed 3712.60 words/sec, time elapsed 2454.12 sec\n",
            "epoch 14, iter 6120, avg. loss 0.31, avg. ppl 1.01 cum. examples 3815, speed 3371.63 words/sec, time elapsed 2456.88 sec\n",
            "epoch 14, iter 6130, avg. loss 0.10, avg. ppl 1.00 cum. examples 4135, speed 3533.91 words/sec, time elapsed 2459.46 sec\n",
            "epoch 14, iter 6140, avg. loss 0.05, avg. ppl 1.00 cum. examples 4455, speed 3627.62 words/sec, time elapsed 2462.01 sec\n",
            "epoch 14, iter 6150, avg. loss 0.04, avg. ppl 1.00 cum. examples 4775, speed 3863.45 words/sec, time elapsed 2464.38 sec\n",
            "epoch 14, iter 6160, avg. loss 0.02, avg. ppl 1.00 cum. examples 5095, speed 3617.42 words/sec, time elapsed 2466.92 sec\n",
            "epoch 14, iter 6170, avg. loss 0.03, avg. ppl 1.00 cum. examples 5415, speed 3559.84 words/sec, time elapsed 2469.53 sec\n",
            "epoch 14, iter 6180, avg. loss 0.03, avg. ppl 1.00 cum. examples 5735, speed 3848.88 words/sec, time elapsed 2472.00 sec\n",
            "epoch 14, iter 6190, avg. loss 0.01, avg. ppl 1.00 cum. examples 6055, speed 3567.73 words/sec, time elapsed 2474.53 sec\n",
            "epoch 14, iter 6200, avg. loss 0.02, avg. ppl 1.00 cum. examples 6375, speed 3950.48 words/sec, time elapsed 2476.90 sec\n",
            "epoch 14, iter 6200, cum. loss 0.07, cum. ppl 1.00 cum. examples 6375\n",
            "begin validation ...\n",
            "validation: iter 6200, dev. ppl 1.001233\n",
            "save currently the best model to [model.bin]\n",
            "save model parameters to [model.bin]\n",
            "epoch 14, iter 6210, avg. loss 0.09, avg. ppl 1.00 cum. examples 320, speed 296.10 words/sec, time elapsed 2508.16 sec\n",
            "epoch 14, iter 6220, avg. loss 0.02, avg. ppl 1.00 cum. examples 640, speed 3405.31 words/sec, time elapsed 2510.88 sec\n",
            "epoch 14, iter 6230, avg. loss 0.02, avg. ppl 1.00 cum. examples 960, speed 3678.82 words/sec, time elapsed 2513.38 sec\n",
            "epoch 14, iter 6240, avg. loss 0.06, avg. ppl 1.00 cum. examples 1280, speed 3623.36 words/sec, time elapsed 2516.00 sec\n",
            "epoch 14, iter 6250, avg. loss 0.17, avg. ppl 1.01 cum. examples 1600, speed 3100.49 words/sec, time elapsed 2519.02 sec\n",
            "epoch 14, iter 6260, avg. loss 0.02, avg. ppl 1.00 cum. examples 1920, speed 3691.20 words/sec, time elapsed 2521.43 sec\n",
            "epoch 14, iter 6270, avg. loss 0.05, avg. ppl 1.00 cum. examples 2240, speed 3657.25 words/sec, time elapsed 2523.91 sec\n",
            "epoch 14, iter 6280, avg. loss 0.06, avg. ppl 1.00 cum. examples 2560, speed 3420.98 words/sec, time elapsed 2526.44 sec\n",
            "epoch 14, iter 6290, avg. loss 0.09, avg. ppl 1.00 cum. examples 2880, speed 3922.66 words/sec, time elapsed 2528.90 sec\n",
            "epoch 14, iter 6300, avg. loss 0.06, avg. ppl 1.00 cum. examples 3200, speed 3505.50 words/sec, time elapsed 2531.49 sec\n",
            "epoch 14, iter 6310, avg. loss 0.02, avg. ppl 1.00 cum. examples 3520, speed 3303.00 words/sec, time elapsed 2534.15 sec\n",
            "epoch 14, iter 6320, avg. loss 0.06, avg. ppl 1.00 cum. examples 3840, speed 3136.03 words/sec, time elapsed 2537.04 sec\n",
            "epoch 14, iter 6330, avg. loss 0.02, avg. ppl 1.00 cum. examples 4160, speed 3585.46 words/sec, time elapsed 2539.65 sec\n",
            "epoch 14, iter 6340, avg. loss 0.01, avg. ppl 1.00 cum. examples 4480, speed 3526.81 words/sec, time elapsed 2542.27 sec\n",
            "epoch 14, iter 6350, avg. loss 0.02, avg. ppl 1.00 cum. examples 4800, speed 3729.38 words/sec, time elapsed 2544.94 sec\n",
            "epoch 14, iter 6360, avg. loss 0.03, avg. ppl 1.00 cum. examples 5120, speed 3580.47 words/sec, time elapsed 2547.57 sec\n",
            "epoch 14, iter 6370, avg. loss 0.02, avg. ppl 1.00 cum. examples 5440, speed 3808.06 words/sec, time elapsed 2549.81 sec\n",
            "epoch 14, iter 6380, avg. loss 0.06, avg. ppl 1.00 cum. examples 5760, speed 3495.30 words/sec, time elapsed 2552.38 sec\n",
            "epoch 14, iter 6390, avg. loss 0.14, avg. ppl 1.00 cum. examples 6080, speed 3311.87 words/sec, time elapsed 2555.14 sec\n",
            "epoch 14, iter 6400, avg. loss 0.08, avg. ppl 1.00 cum. examples 6400, speed 3393.68 words/sec, time elapsed 2557.81 sec\n",
            "epoch 14, iter 6400, cum. loss 0.06, cum. ppl 1.00 cum. examples 6400\n",
            "begin validation ...\n",
            "validation: iter 6400, dev. ppl 1.001278\n",
            "hit patience 1\n",
            "hit #5 trial\n",
            "early stop!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!bash /content/drive/MyDrive/ChrEn/run.sh test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oagHA-er1Gi9",
        "outputId": "7d1a76ae-2820-406b-dfa9-50736d3ba30a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "load test source sentences from [./chr_en_data/test.chr]\n",
            "load test target sentences from [./chr_en_data/test.en]\n",
            "load model from model.bin\n",
            "Decoding: 100% 14855/14855 [10:25<00:00, 23.75it/s]\n",
            "Corpus BLEU: 0.09467879568752796\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R6NBAXK0NkcV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}